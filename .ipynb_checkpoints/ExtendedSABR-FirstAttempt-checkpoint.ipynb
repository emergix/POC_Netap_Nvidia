{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivier/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "import sys, os\n",
    "# identification de la base de prix utilis√©e : directory principal\n",
    "# sur ubuntu\n",
    "\n",
    "path=''\n",
    "if platform == 'linux' :\n",
    "    path ='/workspace/NeuralPricing/A_GeneralizedSABR/CallOption'\n",
    "# sur. mac osx\n",
    "if platform == 'darwin' :\n",
    "    path ='/Users/olivier/keras/NeuralPricing/A_GeneralizedSABR'\n",
    "import keras,tensorflow,pkg_resources\n",
    "dataDirectory='Data'\n",
    "dataLearningFile = \"LearningBaseFileB.CSV\"\n",
    "#datafile='Data/Baby_Learningbase1.CSV'\n",
    "os.chdir(path)\n",
    "# le package identifie la methode de calibration : sous-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from math import sqrt, exp, log, erf,floor\n",
    "import numpy \n",
    "import pandas\n",
    "from decimal import *\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "import keras.layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from IPython.display import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "getcontext().prec = 8\n",
    "\n",
    "from util_functions_ExtendedSABR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=metaparameters()\n",
    "\n",
    "params.INPUT_DIM  = 12\n",
    "params.INPUT_OPTION = 11\n",
    "params.INPUT_VOL =12\n",
    "params.NB_NEURON_PRINCIPAL =8\n",
    "params.ACTIVATION_PRINCIPALE = 'tanh'\n",
    "params.ACTIVATION_PRINCIPALE_FINALE = 'linear'\n",
    "params.INITIAL_LEARNING_NB_EPOCH=1000\n",
    "params.LEARNINGBASE_ORIGIN=\"New_Test\"\n",
    "params.LEARNINGBASE_BUT=\"New_Simu_Test\"\n",
    "params.GENETIC_LEARNING_NB_EPOCH = 10\n",
    "params.BATCH_SIZE_PRINCIPAL = 32768\n",
    "params.OPTIMIZER='adamax'\n",
    "params.OPTIMIZER_GENETIC='SGD'\n",
    "params.NBLAYERS = 11\n",
    "params.NB_LOOPS = 5\n",
    "params.PATH = path\n",
    "params.VERBOSE_FLAG=2\n",
    "params.EPSILON_GREEDINESS = 0.25\n",
    "params.EPSILON_GREEDINESS_DECREASING_FACTOR = 0.99\n",
    "params.INITIAL_NETWORK_STRUCTURE = [[0,10],[1,20],[2,30],[0,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps de lecture du ficher 5.91037392616272\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "dataframe = pandas.read_csv(dataDirectory+\"/\"+dataLearningFile)\n",
    "endTime = time.time()\n",
    "print('temps de lecture du ficher',endTime - startTime)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,1:params.INPUT_DIM+1]\n",
    "datasize=dataset[1:,params.INPUT_DIM].size\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = (X_scaler.fit_transform(X))\n",
    "Y_Vol = dataset[:,params.INPUT_VOL]\n",
    "params.X_scaler = X_scaler\n",
    "params.X_scaled = X_scaled\n",
    "params.X = X\n",
    "Y_scaler_Vol = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaled_Vol = (Y_scaler_Vol.fit_transform(Y_Vol.reshape(-1, 1))).reshape(1, -1)[0]\n",
    "params.Y_scaler_Vol = Y_scaler_Vol\n",
    "params.Y_scaled_Vol = Y_scaled_Vol\n",
    "params.Y_Vol = Y_Vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Y_ordre1_Vol,model_Initial,y_scaler_Vol,y_scaled_Vol=InitialCalibration2(params.Y_Vol,\"Vol\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 3\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 1s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 1s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 1s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 1s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 1s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 1s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 1s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 1s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 1s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 1s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  13.88791298866272\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 1\n",
      "Note = 2.714355286238188\n",
      "notefinale= 2.714355286238188\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 2s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 2s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 2s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  19.790898084640503\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 2\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 2s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 2s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 2s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  22.972776889801025\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 3\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 3s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 2s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  25.22694969177246\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 4\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 3s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 3s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 3s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  27.566237926483154\n",
      "starting to save\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk as Vol\n",
      "number of additional learning : 5\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 3s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 2s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 2s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  25.04571771621704\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 6\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 3s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 3s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 3s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 3s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  29.15138292312622\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 7\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 4s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 3s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 3s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 3s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  31.6900851726532\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 8\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 0\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 4s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 3s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 3s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 3s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n",
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  34.9160099029541\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 9\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n",
      "Note = 2.7149592700632654\n",
      "noteinitiale= 2.7149592700632654\n",
      "i= 3\n",
      "NbLayers= 4\n",
      "New model synthesized\n",
      "starting fit\n",
      "Epoch 1/10\n",
      " - 4s - loss: 7.4705e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.2062e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00002: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 7.1368e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00003: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 7.0819e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 5/10\n",
      " - 3s - loss: 7.0347e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 6/10\n",
      " - 3s - loss: 6.9945e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 6.9604e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00007: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 8/10\n",
      " - 3s - loss: 6.9328e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00008: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 9/10\n",
      " - 3s - loss: 6.9120e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00009: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 6.8954e-09 - acc: 1.2439e-06\n",
      "\n",
      "Epoch 00010: loss improved from 0.00000 to 0.00000, saving model to BestWeights.hdf5\n",
      "temps de calibration  34.40053606033325\n",
      "starting to save\n",
      "Saved model to disk as Vol\n",
      "number of additional learning : 10\n",
      "Note = 2.714355286724261\n",
      "notefinale= 2.714355286724261\n"
     ]
    }
   ],
   "source": [
    "nbloop =10\n",
    "pkgNameOriginal = params.LEARNINGBASE_ORIGIN\n",
    "pkgNameBut = params.LEARNINGBASE_BUT\n",
    "Y_ordre1,model2,y_scaler,y_scaled,resultats=ReinforceOptimality2(pkgNameOriginal,pkgNameBut,nbloop,params.Y_Vol,\"Vol\",params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des Modules d'Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Layer0 (Dense)                  (None, 10)           130         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type11 (Dense)      (None, 20)           220         Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier1 (Dense) (None, 10)           200         ResnetLayer_type11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 10)           0           ResnetLayer_Multiplier1[0][0]    \n",
      "                                                                 Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type22 (Dense)      (None, 30)           330         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier2 (Dense) (None, 30)           300         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 30)           0           ResnetLayer_type22[0][0]         \n",
      "                                                                 ResnetLayer_Multiplier2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Layer3 (Dense)                  (None, 10)           310         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          Layer3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,501\n",
      "Trainable params: 1,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "speclist =[[0,10],[1,20],[2,30],[0,10]]\n",
    "mm =buidModel(speclist,params)\n",
    "print(mm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Layer0 (Dense)                  (None, 13)           169         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type11 (Dense)      (None, 24)           336         Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier1 (Dense) (None, 13)           312         ResnetLayer_type11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 13)           0           ResnetLayer_Multiplier1[0][0]    \n",
      "                                                                 Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type22 (Dense)      (None, 35)           490         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier2 (Dense) (None, 35)           455         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 35)           0           ResnetLayer_type22[0][0]         \n",
      "                                                                 ResnetLayer_Multiplier2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Layer3 (Dense)                  (None, 16)           576         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            17          Layer3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,355\n",
      "Trainable params: 2,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "deltaspecList=[3,4,5,6,7]\n",
    "mm1 =injectionModel(mm,speclist,deltaspecList,params) \n",
    "print(mm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnfigName=\"Vol\"\n",
    "pkgPath=\"New_Test\"\n",
    "loaded_model,X_scaler_loaded,y_scaler_loaded,y_scaled_loaded,Y_ordre1_loaded,loaded_speclist = reloadModel(cnfigName,pkgPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note = 2.7149592700632654\n",
      "Note = 2.7149592706086243\n"
     ]
    }
   ],
   "source": [
    "deltaspecList=[0,10,0,0]\n",
    "mm1 =injectionModel(loaded_model,loaded_speclist,deltaspecList,params) \n",
    "##print(mm1.summary())\n",
    "Compute_Note(loaded_model,[params.PATH+ '/'+'Data/Notation.CSV'],params,y_scaler_loaded)\n",
    "Compute_Note(mm1,[params.PATH+ '/'+'Data/Notation.CSV'],params,y_scaler_loaded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note = 2.7149592700632654\n",
      "Note = 2.7149592700632654\n"
     ]
    }
   ],
   "source": [
    "ResnetRank=2\n",
    "ResnetWeight=35\n",
    "ResnetType=1\n",
    "mm2=injectionModelWithInsertion(loaded_model,loaded_speclist,ResnetRank,ResnetWeight,ResnetType,params)\n",
    "##print(mm2.summary())\n",
    "Compute_Note(loaded_model,[params.PATH+ '/'+'Data/Notation.CSV'],params,y_scaler_loaded)\n",
    "Compute_Note(mm2,[params.PATH+ '/'+'Data/Notation.CSV'],params,y_scaler_loaded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildParamList(modelspecList,params) : \n",
    "    model = buidModel(modelspecList,params)   \n",
    "    weights1=model.get_weights()\n",
    "    return list(map(lambda k: k.shape ,weights1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Layer0 (Dense)                  (None, 10)           130         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type11 (Dense)      (None, 20)           220         Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier1 (Dense) (None, 10)           200         ResnetLayer_type11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 10)           0           ResnetLayer_Multiplier1[0][0]    \n",
      "                                                                 Layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type12 (Dense)      (None, 35)           385         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier2 (Dense) (None, 10)           350         ResnetLayer_type12[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 10)           0           ResnetLayer_Multiplier2[0][0]    \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_type23 (Dense)      (None, 30)           330         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ResnetLayer_Multiplier3 (Dense) (None, 30)           300         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 30)           0           ResnetLayer_type23[0][0]         \n",
      "                                                                 ResnetLayer_Multiplier3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Layer4 (Dense)                  (None, 10)           310         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          Layer4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,236\n",
      "Trainable params: 2,236\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ResnetRank=2\n",
    "ResnetWeight=35\n",
    "ResnetType=1\n",
    "mm2=injectionModelWithInsertion(loaded_model,loaded_speclist,ResnetRank,ResnetWeight,ResnetType,params)\n",
    "print(mm2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParametersList = [\"alpha0\",\"beta\",\"beta2\",\"d\",\"gamma\",\"nu\",\"omega\",\"lambda\",\"rho\",\"maturity\",\"strike\",\"option\",\"vol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountIndividuals(a)    :\n",
    "    ii=0\n",
    "    for f in os.listdir(a):\n",
    "        ii=ii+1\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergeIndividualModel(model2):\n",
    "    print('convergeIndividualModel :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeBestNoteIndexlist(ListOfnote,populationctrlFunc,iTimeStep):\n",
    "    lenlist = len(ListOfnote)\n",
    "    sortedlist = np.argsort(ListOfnote)\n",
    "    lim = populationctrlFunc(iTimeStep)\n",
    "    lim = min(lenlist,lim)\n",
    "    print('lim=',lim)\n",
    "    ListOfWorst=sortedlist[lim:] \n",
    "    ListOfBest=sortedlist[:lim]\n",
    "    return ListOfWorst,ListOfBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathCreatechild(TimePath,individualpath,child):\n",
    "    pathCreated = TimePath + '/individu' +  str(child)\n",
    "    if not(os.path.isdir(pathCreated)):\n",
    "                os.makedirs(pathCreated,0o777 )\n",
    "    print('pathCreatechild=',pathCreated)\n",
    "    return pathCreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def killIndividual(individualPath):\n",
    "    print(\"killing :\",individualPath)\n",
    "    shutil.rmtree(individualPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ComputeBestNoteIndexlist() missing 1 required positional argument: 'iTimeStep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-c505213bf6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mComputeBestNoteIndexlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ComputeBestNoteIndexlist() missing 1 required positional argument: 'iTimeStep'"
     ]
    }
   ],
   "source": [
    "ll=[3,4,6.5,2.3,7.5,3.4,6.8,1.2,1.4,1.8]\n",
    "ComputeBestNoteIndexlist(ll,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveIndividualModel(model,X_scaler,y_scaler,y_scaled,Y_ordre1,specList,path,cnfigName):\n",
    "    model.save_weights(path + '/' + cnfigName +'.hdf5')\n",
    "    # save model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(path + '/' + cnfigName +'.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # Save scaler Xand Y\n",
    "    filename = path + '/' + cnfigName +'scalerX.pkl'\n",
    "    _= joblib.dump(X_scaler , filename , compress = 9)\n",
    "    filename = path + '/' + cnfigName +'scalerY.pkl'\n",
    "    _= joblib.dump(y_scaler , filename , compress = 9)\n",
    "    filename = path + '/' + cnfigName +'scaledY.pkl'\n",
    "    _= joblib.dump(y_scaled , filename , compress = 9)\n",
    "    filename = path + '/' + cnfigName +'Y_ordre1.res'\n",
    "    _= joblib.dump(Y_ordre1 , filename , compress = 9)\n",
    "    filename = path + '/' + cnfigName +'current_structure.str'\n",
    "    _= joblib.dump(specList , filename , compress = 9)\n",
    "    print('saveIndividualModel model2 at :',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInvidualModel(IndividuPath,cnfigName):\n",
    "    json_file = open(IndividuPath+ '/' + cnfigName  +'.json', 'r')\n",
    "    file = IndividuPath+ '/' + cnfigName  +'.json'\n",
    "    print(\"trying to get model from :\",file)\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()  \n",
    "    loaded_model = model_from_json(loaded_model_json)           \n",
    "    filename2 = IndividuPath+ '/' + cnfigName  +'current_structure.str'\n",
    "    specList = joblib.load(filename2 )\n",
    "    filename2 = IndividuPath+ '/' + cnfigName  +'scalerY.pkl'\n",
    "    y_scaler = joblib.load(filename2)\n",
    "    filename2 = IndividuPath+ '/' + cnfigName  +'scaledY.pkl'\n",
    "    y_scaled = joblib.load(filename2)\n",
    "    print('loadInvidualModel :IndividuPath=',IndividuPath)\n",
    "    return loaded_model, specList,y_scaler,y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutateIndividualModel(model,speclist):\n",
    "    # specList = [[specList[i][0],specList[i][1]+  deltaNbNeuronList[i] ] for i in range(NbLayers)]\n",
    "    print('mutateIndividualModel :')\n",
    "    return model,speclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyModelFromInitial(originalModelPath,IndividuPath):\n",
    "    shutil.copyfile(originalModelPath+'current_structure.str',IndividuPath+'current_structure.str')\n",
    "    shutil.copyfile(originalModelPath+'scalerY.pkl',IndividuPath+'scalerY.pkl')\n",
    "    shutil.copyfile(originalModelPath+'scaledY.pkl',IndividuPath+'scaledY.pkl')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createResidues(X,X_scaler,model,y_scaler,Ycall,generate_errors):\n",
    "    Y_size = X.shape[0]\n",
    "    print(\"creating the residus for the boosting\")\n",
    "    Y_ordre1=numpy.zeros(Ycall.size); \n",
    "    if (generate_errors):        \n",
    "        for i in range(Y_size):\n",
    "            normalizedData=X_scaler.transform(X[i].reshape(1, -1))\n",
    "            normalizedPrediction=model.predict(normalizedData)\n",
    "            outputData=y_scaler.inverse_transform(normalizedPrediction)\n",
    "            Y_ordre1[i]=outputData[0,0]-Ycall[i]\n",
    "        return Y_ordre1\n",
    "    else:\n",
    "        return Y_ordre1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeNote(individualpath,cnfigName,params,y_scaler):\n",
    "    file = individualpath +'/'+cnfigName +'.json'\n",
    "    os.chdir(individualpath)\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()              \n",
    "    model = model_from_json(loaded_model_json)              ### chargment du model initial (1)\n",
    "    file = individualpath +'/'+cnfigName +'.hdf5'\n",
    "    model.load_weights(file)  ### chargment du model initial (2)\n",
    "    note = Compute_Note(model,[params.PATH+ '/'+'Data/Notation.CSV'],params,y_scaler)\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/olivier/keras/NeuralPricing/A_GeneralizedSABR'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReinforceOptimality3(pkgNameOriginal,pkgNameBut,InitialNbIndividual,populationctrlFunc,nbChildAllowed,\\\n",
    "                         nbloop,Ycall,cnfigName,params,save=True,generate_errors = False):\n",
    "    originalModelPath=params.PATH + \"/\" + pkgNameOriginal   \n",
    "    filename = originalModelPath + '/' + cnfigName +'scaledY.pkl'\n",
    "    y_scaled = joblib.load(filename)\n",
    "    filename = originalModelPath + '/' + cnfigName +'scalerY.pkl'\n",
    "    y_scaler = joblib.load(filename)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split( \\\n",
    "                params.X_scaled, y_scaled, test_size=0.10, random_state=3)\n",
    "    SimuPath = params.PATH + \"/\" + pkgNameBut \n",
    "    if not(os.path.isdir(SimuPath)):\n",
    "                os.makedirs(SimuPath,0o777 )\n",
    "    SimuPath = SimuPath + '/loop'\n",
    "    if (os.path.isdir(SimuPath)):\n",
    "        shutil.rmtree(SimuPath)\n",
    "    os.makedirs(SimuPath,0o777 ) \n",
    "    precedingIndividualPathList =[]\n",
    "    for iTimeStep in range(nbloop):\n",
    "        TimePath = SimuPath + \"/\" + \"time\"+str(iTimeStep) \n",
    "        if not(os.path.isdir(TimePath)):\n",
    "                os.makedirs(TimePath,0o777 )\n",
    "        if (iTimeStep == 0) :\n",
    "            for individual in range(InitialNbIndividual):\n",
    "                IndividuPath= TimePath +  \"/individu\" + str(individual)\n",
    "                if not(os.path.isdir(IndividuPath)):\n",
    "                        os.makedirs(IndividuPath,0o777 )\n",
    "                ## ....traitement\n",
    "                model,specList,y_scaler,y_scaled = loadInvidualModel(originalModelPath,cnfigName)\n",
    "                model2,specList2=mutateIndividualModel(model,specList)\n",
    "                convergeIndividualModel(model2)\n",
    "                Y_ordre1 = createResidues(params.X,X_scaler,model2,y_scaler,Ycall,generate_errors)\n",
    "                saveIndividualModel(model2,X_scaler,y_scaler,y_scaled,Y_ordre1,specList,IndividuPath,cnfigName)\n",
    "               \n",
    "                ## .. Fin du traitement\n",
    "                precedingIndividualPathList.append(IndividuPath)\n",
    "                        \n",
    "        else :\n",
    "            invidualpathList = []\n",
    "            individuNumero =0\n",
    "            for individualpath in precedingIndividualPathList:                            \n",
    "            ## ....traitement\n",
    "                model,specList,y_scaler,y_scaled = loadInvidualModel(individualpath,cnfigName)\n",
    "                for child in range(nbChildAllowed): \n",
    "                    childPath = pathCreatechild(TimePath,individualpath,individuNumero)\n",
    "                    individuNumero +=1\n",
    "                    model2,specList2=mutateIndividualModel(model,specList)\n",
    "                    convergeIndividualModel(model2)\n",
    "                    Y_ordre1 = createResidues(params.X,X_scaler,model2,y_scaler,Ycall,generate_errors)\n",
    "                    saveIndividualModel(model2,X_scaler,y_scaler,y_scaled,Y_ordre1,specList2,childPath,cnfigName)\n",
    "                    invidualpathList.append(childPath)\n",
    "                precedingIndividualPathList = invidualpathList\n",
    "            ## .. Fin du traitement\n",
    "        ## .. Debut Selection Naturelle\n",
    "        ListOnote =[]\n",
    "        invidualpathList = precedingIndividualPathList\n",
    "        for individualpath in invidualpathList :\n",
    "            note = ComputeNote(individualpath,cnfigName,params,y_scaler)\n",
    "            ListOnote.append(note)\n",
    "        worstNoteIndexlist,bestNoteIndexlist = ComputeBestNoteIndexlist(ListOnote,populationctrlFunc,iTimeStep)\n",
    "        print('ListOnote=',ListOnote)\n",
    "        print('bestNoteIndexlist=',bestNoteIndexlist)\n",
    "        print('worstNoteIndexlist=',worstNoteIndexlist)\n",
    "        for index in worstNoteIndexlist : \n",
    "            killIndividual(invidualpathList[index])\n",
    "        precedingIndividualPathList = [invidualpathList[index] for index in bestNoteIndexlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Idealpopulation(nstep):\n",
    "    if (nstep<4): \n",
    "        return 3*nstep+2\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get model from : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test/Vol.json\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu0\n",
      "trying to get model from : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test/Vol.json\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu1\n",
      "trying to get model from : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test/Vol.json\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Test\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu2\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu0/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu0/Vol.json\n",
      "Note = 15.126729089134615\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu1/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu1/Vol.json\n",
      "Note = 13.189987321024432\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu2/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu2/Vol.json\n",
      "Note = 9.933644460709568\n",
      "lim= 2\n",
      "ListOnote= [15.126729089134615, 13.189987321024432, 9.933644460709568]\n",
      "bestNoteIndexlist= [2 1]\n",
      "worstNoteIndexlist= [0]\n",
      "killing : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu0\n",
      "trying to get model from : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu2/Vol.json\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu2\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu0\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu0\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu1\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu1\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu2\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu2\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu3\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu3\n",
      "trying to get model from : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu1/Vol.json\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time0/individu1\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu4\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu4\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu5\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu5\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu6\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu6\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu7\n",
      "mutateIndividualModel :\n",
      "convergeIndividualModel :\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu7\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu0/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu0/Vol.json\n",
      "Note = 17.204149263434147\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu1/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu1/Vol.json\n",
      "Note = 17.204149263434147\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu2/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu2/Vol.json\n",
      "Note = 17.204149263434147\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu3/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu3/Vol.json\n",
      "Note = 17.204149263434147\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu4/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu4/Vol.json\n",
      "Note = 25.778949423360782\n",
      " ####### looking at path : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu5/Vol.json\n",
      "6666666666 looking for : /Users/olivier/keras/NeuralPricing/A_GeneralizedSABR/New_Simu_Test/loop/time1/individu5/Vol.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3fb50be3266a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpkgNameBut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNINGBASE_BUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mInitialNbIndividual\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mnaturalSelectionPercentage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReinforceOptimality3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkgNameOriginal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpkgNameBut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInitialNbIndividual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIdealpopulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnbChildAllowed\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0mnbloop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Vol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Vol\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-d7fdc4dd081c>\u001b[0m in \u001b[0;36mReinforceOptimality3\u001b[0;34m(pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed, nbloop, Ycall, cnfigName, params, save, generate_errors)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0minvidualpathList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecedingIndividualPathList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindividualpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvidualpathList\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mnote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividualpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnfigName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mListOnote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mworstNoteIndexlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbestNoteIndexlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeBestNoteIndexlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListOnote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpopulationctrlFunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miTimeStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-fdf47e0e41cd>\u001b[0m in \u001b[0;36mComputeNote\u001b[0;34m(individualpath, cnfigName, params, y_scaler)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividualpath\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcnfigName\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m### chargment du model initial (2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompute_Note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Data/Notation.CSV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/NeuralPricing/A_GeneralizedSABR/util_functions_ExtendedSABR.py\u001b[0m in \u001b[0;36mCompute_Note\u001b[0;34m(model_Option, csvlist, params, y_scaler_Option)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mXNotation_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXNotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mYcall_Notation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetNotation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINPUT_GOAL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mnormedOptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_scaler_Option\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_Option\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXNotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0mnormederrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYcall_Notation\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnormedOptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mpriceerrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYcall_Notation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormedOptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nbloop =5;NbIndividual =3; nbChildAllowed =4\n",
    "pkgNameOriginal = params.LEARNINGBASE_ORIGIN\n",
    "pkgNameBut = params.LEARNINGBASE_BUT\n",
    "InitialNbIndividual =3;naturalSelectionPercentage =0.5\n",
    "a=ReinforceOptimality3(pkgNameOriginal,pkgNameBut,InitialNbIndividual,Idealpopulation,nbChildAllowed,\\\n",
    "                       nbloop,Y_Vol,\"Vol\",params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
