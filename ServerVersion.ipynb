{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "\n",
    "from sys import platform\n",
    "from decimal import *\n",
    "\n",
    "\n",
    "path='./'\n",
    "if platform == 'win32' :\n",
    "    #path ='/workspace'\n",
    "    path ='C:/Users/Sergey/Desktop/Natixis/Yeti'\n",
    "\n",
    "\n",
    "import keras, tensorflow, pkg_resources\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from math import sqrt, exp, log, erf,floor\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "import keras.layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from IPython.display import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import threading\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "# Generate dummy data\n",
    "\n",
    "\n",
    "\n",
    "getcontext().prec = 8\n",
    "from util_functions_YetiPhen_VolLoc import *\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "ListField1 ={\"S1\", \"mu1\", \"bonus\", \"YetiBarrier\", \"YetiCoupon\", \"PhoenixBarrier\",\"PhoenixCoupon\",\"PDIBarrier\",\"PDIGearing\",\"PDIStrike\",\"PDIType\",\n",
    "             \"maturity\",\"nbDates\"}\n",
    "\n",
    "ListField2={\"vol-date0-strike0\",\"vol-date0-strike1\",\"vol-date0-strike2\",\"vol-date0-strike3\",\"vol-date0-strike4\",\"vol-date0-strike5\",\"vol-date0-strike6\",\n",
    "           \"vol-date0-strike7\",\"vol-date0-strike8\",\"vol-date0-strike9\",\"vol-date0-strike10\",\"vol-date0-strike11\",\"vol-date0-strike12\",\"vol-date0-strike13\",\n",
    "           \"vol-date0-strike14\",\"vol-date1-strike0\",\"vol-date1-strike1\",\"vol-date1-strike2\",\"vol-date1-strike3\",\"vol-date1-strike4\",\"vol-date1-strike5\",\n",
    "           \"vol-date1-strike6\",\"vol-date1-strike7\",\"vol-date1-strike8\",\"vol-date1-strike9\",\"vol-date1-strike10\",\"vol-date1-strike11\",\"vol-date1-strike12\",\n",
    "           \"vol-date1-strike13\",\"vol-date1-strike14\",\"vol-date2-strike0\",\"vol-date2-strike1\",\"vol-date2-strike2\",\"vol-date2-strike3\",\"vol-date2-strike4\",\n",
    "           \"vol-date2-strike5\",\"vol-date2-strike6\",\"vol-date2-strike7\",\"vol-date2-strike8\",\"vol-date2-strike9\",\"vol-date2-strike10\",\"vol-date2-strike11\",\n",
    "           \"vol-date2-strike12\",\"vol-date2-strike13\",\"vol-date2-strike14\",\"vol-date3-strike0\",\"vol-date3-strike1\",\"vol-date3-strike2\",\"vol-date3-strike3\",\n",
    "           \"vol-date3-strike4\",\"vol-date3-strike5\",\"vol-date3-strike6\",\"vol-date3-strike7\",\"vol-date3-strike8\",\"vol-date3-strike9\",\"vol-date3-strike10\",\n",
    "           \"vol-date3-strike11\",\"vol-date3-strike12\",\"vol-date3-strike13\",\"vol-date3-strike14\",\"vol-date4-strike0\",\"vol-date4-strike1\",\"vol-date4-strike2\",\n",
    "           \"vol-date4-strike3\",\"vol-date4-strike4\",\"vol-date4-strike5\",\"vol-date4-strike6\",\"vol-date4-strike7\",\"vol-date4-strike8\",\"vol-date4-strike9\",\n",
    "           \"vol-date4-strike10\",\"vol-date4-strike11\",\"vol-date4-strike12\",\"vol-date4-strike13\",\"vol-date4-strike14\",\"vol-date5-strike0\",\"vol-date5-strike1\",\n",
    "           \"vol-date5-strike2\",\"vol-date5-strike3\",\"vol-date5-strike4\",\"vol-date5-strike5\",\"vol-date5-strike6\",\"vol-date5-strike7\",\"vol-date5-strike8\",\n",
    "           \"vol-date5-strike9\",\"vol-date5-strike10\",\"vol-date5-strike11\",\"vol-date5-strike12\",\"vol-date5-strike13\",\"vol-date5-strike14\",\"vol-date6-strike0\",\n",
    "           \"vol-date6-strike1\",\"vol-date6-strike2\",\"vol-date6-strike3\",\"vol-date6-strike4\",\"vol-date6-strike5\",\"vol-date6-strike6\",\"vol-date6-strike7\",\n",
    "           \"vol-date6-strike8\",\"vol-date6-strike9\",\"vol-date6-strike10\",\"vol-date6-strike11\",\"vol-date6-strike12\",\"vol-date6-strike13\",\"vol-date6-strike14\",\n",
    "           \"vol-date7-strike0\",\"vol-date7-strike1\",\"vol-date7-strike2\",\"vol-date7-strike3\",\"vol-date7-strike4\",\"vol-date7-strike5\",\"vol-date7-strike6\",\n",
    "           \"vol-date7-strike7\",\"vol-date7-strike8\",\"vol-date7-strike9\",\"vol-date7-strike10\",\"vol-date7-strike11\",\"vol-date7-strike12\",\"vol-date7-strike13\",\n",
    "           \"vol-date7-strike14\",\"vol-date8-strike0\",\"vol-date8-strike1\",\"vol-date8-strike2\",\"vol-date8-strike3\",\"vol-date8-strike4\",\"vol-date8-strike5\",\n",
    "           \"vol-date8-strike6\",\"vol-date8-strike7\",\"vol-date8-strike8\",\"vol-date8-strike9\",\"vol-date8-strike10\",\"vol-date8-strike11\",\"vol-date8-strike12\",\n",
    "           \"vol-date8-strike13\",\"vol-date8-strike14\",\"vol-date9-strike0\",\"vol-date9-strike1\",\"vol-date9-strike2\",\"vol-date9-strike3\",\"vol-date9-strike4\",\n",
    "           \"vol-date9-strike5\",\"vol-date9-strike6\",\"vol-date9-strike7\",\"vol-date9-strike8\",\"vol-date9-strike9\",\"vol-date9-strike10\",\"vol-date9-strike11\",\n",
    "           \"vol-date9-strike12\",\"vol-date9-strike13\",\"vol-date9-strike14\",\"vol-date10-strike0\",\"vol-date10-strike1\",\"vol-date10-strike2\",\"vol-date10-strike3\",\n",
    "           \"vol-date10-strike4\",\"vol-date10-strike5\",\"vol-date10-strike6\",\"vol-date10-strike7\",\"vol-date10-strike8\",\"vol-date10-strike9\",\"vol-date10-strike10\",\n",
    "           \"vol-date10-strike11\",\"vol-date10-strike12\",\"vol-date10-strike13\",\"vol-date10-strike14\",\"vol-date11-strike0\",\"vol-date11-strike1\",\"vol-date11-strike2\",\n",
    "           \"vol-date11-strike3\",\"vol-date11-strike4\",\"vol-date11-strike5\",\"vol-date11-strike6\",\"vol-date11-strike7\",\n",
    "           \"vol-date11-strike8\",\"vol-date11-strike9\",\"vol-date11-strike10\",\"vol-date11-strike11\",\"vol-date11-strike12\",\"vol-date11-strike13\",\"vol-date11-strike14\"}\n",
    "\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "params = metaparameters()\n",
    "\n",
    "params.INPUT_DIM  = 193\n",
    "params.INPUT_OPTION = 193\n",
    "params.INPUT_VOL = 13\n",
    "params.NB_NEURON_PRINCIPAL = 8\n",
    "params.ACTIVATION_PRINCIPALE = 'tanh'\n",
    "params.ACTIVATION_PRINCIPALE_FINALE = 'linear'\n",
    "params.INITIAL_LEARNING_NB_EPOCH = 100\n",
    "params.LEARNINGBASE_ORIGIN = \"New_Test\"\n",
    "params.LEARNINGBASE_BUT = \"New_Simu_Test\"\n",
    "params.GENETIC_LEARNING_NB_EPOCH = 10\n",
    "params.BATCH_SIZE_PRINCIPAL = 32768\n",
    "params.OPTIMIZER = 'adamax'##############################\n",
    "params.OPTIMIZER_GENETIC = 'SGD'###########################\n",
    "params.NBLAYERS = 11\n",
    "params.NB_LOOPS = 5\n",
    "params.PATH = path\n",
    "params.VERBOSE_FLAG = 2\n",
    "params.EPSILON_GREEDINESS = 0.25\n",
    "params.EPSILON_GREEDINESS_DECREASING_FACTOR = 0.99\n",
    "params.INITIAL_NETWORK_STRUCTURE = [[0,10], [1,20], [2,30], [0,10]]\n",
    "params.NOTATION_FILE = dataDirectory + '/' + dataNotationFile\n",
    "params.LISTFIELD1 = ListField1\n",
    "params.LISTFIELD2 = ListField2\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "path_to = '/mnt/natixis_1/1/'\n",
    "\n",
    "params.list_gen = [path_to + x for x in os.listdir(path_to)]\n",
    "\n",
    "def read_and_scale(addr):\n",
    "    \n",
    "    data = pd.read_csv(addr, sep = ';')\n",
    "    data = data.reset_index(drop = 'index')\n",
    "    \n",
    "    Y = data_temp.price.values\n",
    "    X = data_temp.drop(columns = ['nbDates', 'price']).values\n",
    "    \n",
    "    Y_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "    Y_scaler.fit(Y)\n",
    "    Y_scaled = Y_scaler.transform(Y)\n",
    "       \n",
    "    X_scaler = preprocessing.MinMaxScaler(feature_range = (-1, 1))\n",
    "    X_scaler.fit(X)\n",
    "    X_scaled = X_scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size = 0.2)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X_scaler, Y_scaler\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "X_train, X_test, Y_train, Y_test, X_scaler, Y_scaler = read_and_scale(params.list_gen)\n",
    "\n",
    "params.X_scaler = X_scaler\n",
    "params.Y_scaler = Y_scaler\n",
    "\n",
    "params.X_train = X_train\n",
    "params.X_test = X_test\n",
    "params.Y_train = Y_train\n",
    "params.Y_test = Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps de lecture du ficher 0.03606462478637695\n",
      "temps de lecture du ficher 0.032024383544921875\n"
     ]
    }
   ],
   "source": [
    "def InitialCalibration2(cnfigName, save = True):\n",
    "    \n",
    "    speclist = params.INITIAL_NETWORK_STRUCTURE\n",
    "    model = buidModel(speclist, params)\n",
    "       \n",
    "    #model = keras.utils.multi_gpu_model(model, gpus = 1)\n",
    "    \n",
    "    model.compile(loss = 'mse', optimizer = 'adam', metrics = [\"accuracy\"])\n",
    "    \n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(filepath = \"BestWeights.hdf5\",verbose=1,\\\n",
    "                                                   save_best_only=True , monitor = 'loss')\n",
    "    numpy.random.seed(3)\n",
    "    \n",
    "    \n",
    "    model.fit(params.X_train, params.Y_train, epochs = params.INITIAL_LEARNING_NB_EPOCH,\\\n",
    "              batch_size = params.BATCH_SIZE_PRINCIPAL,\\\n",
    "              verbose = params.VERBOSE_FLAG, shuffle = True, callbacks = [checkpointer])\n",
    "    \n",
    "    model.load_weights(\"BestWeights.hdf5\")\n",
    "\n",
    "    print(\"starting to save\")\n",
    "    if (save):\n",
    "        if not(os.path.isdir(params.LEARNINGBASE_ORIGIN)):\n",
    "                os.makedirs(params.LEARNINGBASE_ORIGIN, 0o777)\n",
    "                \n",
    "        model.save_weights(params.LEARNINGBASE_ORIGIN + '/' + cnfigName +'.hdf5')\n",
    "        \n",
    "        # save model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(params.LEARNINGBASE_ORIGIN + '/' + cnfigName +'.json', 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        filename = params.LEARNINGBASE_ORIGIN + '/' + cnfigName +'current_structure.str'\n",
    "        _= joblib.dump(speclist , filename , compress = 9)\n",
    "        \n",
    "        print(\"Saved model to disk as \" + cnfigName)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = InitialCalibration2('Vol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_N(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_FICHIERS, n_dims = 16, size_FICHIERS = 262144, batch_size = 32768, shuffle=True, mode = 'train'):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.list_FICHIERS = list_FICHIERS\n",
    "        \n",
    "        self.n_fichiers = len(self.list_FICHIERS)\n",
    "        \n",
    "        self.size_fichiers = size_FICHIERS\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "               \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        assert(mode in {'train', 'val', 'test'})\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.current_fichier = 0\n",
    "        \n",
    "        self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    " \n",
    "        self.batches_per_fichier = size_FICHIERS // batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_fichiers * self.size_fichiers / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        \n",
    "        if index // self.batches_per_fichier != self.current_fichier:\n",
    "            self.current_fichier = index // self.batches_per_fichier\n",
    "            \n",
    "            \n",
    "            self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "            self.data = self.data.reset_index(drop = 'index')\n",
    "        \n",
    "        intra_index = index % self.batches_per_fichier\n",
    "\n",
    "        data_temp = self.data.loc[intra_index * self.batch_size : (intra_index + 1) * self.batch_size - 1]\n",
    "        \n",
    "        Y = params.Y_scaler.transform(data_temp.price.values)\n",
    "        X = params.X_scaler.transform(data_temp.drop(columns = ['nbDates', 'price']).values)\n",
    "\n",
    "        \n",
    "        if self.mode in {'train', 'val'}:\n",
    "            Y = Y.reshape((-1, 1))\n",
    "            Y = np.squeeze(np.repeat(Y[:, np.newaxis, :], self.n_dims, axis = 1))\n",
    "        else:\n",
    "            Y = [Y for i in range(self.n_dims)]\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_FICHIERS)\n",
    "\n",
    "        self.current_fichier = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to = '/workspace/FirstAttempt/Data/Sergey/'\n",
    "#path_to = 'C:/Users/Sergey/Desktop/Natixis/DataSamples/'\n",
    "\n",
    "#list_gen = [path_to + str(i) + '.csv' for i in range(5)]\n",
    "\n",
    "\n",
    "TRAIN_GENs = {i : DataGenerator_N(list_gen[1:31], n_dims = i, mode = 'train') for i in params.RANGE}\n",
    "VAL_GENs = {i : DataGenerator_N(list_gen[31:33], n_dims = i, mode = 'val') for i in params.RANGE}\n",
    "TEST_GENs = {i : DataGenerator_N([list_gen[33]], n_dims = i, mode = 'test') for i in params.RANGE}\n",
    "\n",
    "#dg = DataGenerator_N(list_gen[:3], n_dims = 10, mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildParamList(modelspecList, params):\n",
    "    \n",
    "    model = buidModel(modelspecList, params)   \n",
    "    weights1 = model.get_weights()\n",
    "    \n",
    "    return list(map(lambda k: k.shape, weights1))\n",
    "\n",
    "def CountIndividuals(a):\n",
    "    \n",
    "    ii = 0\n",
    "    for f in os.listdir(a):\n",
    "        ii = ii + 1\n",
    "        \n",
    "    return ii\n",
    "\n",
    "def convergeIndividualModel(model2):\n",
    "    \n",
    "    print('convergeIndividualModel :')\n",
    "    \n",
    "    return None\n",
    "   \n",
    "\n",
    "def pathCreatechild(TimePath, individualpath, child):\n",
    "    \n",
    "    pathCreated = TimePath + '/individu' +  str(child)\n",
    "    \n",
    "    if not(os.path.isdir(pathCreated)):\n",
    "        os.makedirs(pathCreated, 0o777)\n",
    "        \n",
    "    print('pathCreatechild=', pathCreated)\n",
    "    \n",
    "    return pathCreated\n",
    "\n",
    "def killIndividual(individualPath):\n",
    "    \n",
    "    print(\"killing :\", individualPath)\n",
    "    shutil.rmtree(individualPath)\n",
    "    \n",
    "def saveIndividualModel(model, specList, path, cnfigName):\n",
    "    \n",
    "    model.save_weights(path + '/' + cnfigName + '.hdf5')\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(path + '/' + cnfigName +'.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    filename = path + '/' + cnfigName + 'current_structure.str'\n",
    "    _= joblib.dump(specList, filename, compress = 9)\n",
    "    \n",
    "    print('saveIndividualModel model2 at :', path)\n",
    "\n",
    "    \n",
    "def loadInvidualModel(IndividuPath, cnfigName):\n",
    "    \n",
    "    json_file = open(IndividuPath + \"/\" + cnfigName  + '.json', 'r')\n",
    "    file = IndividuPath + '/' + cnfigName  + '.json'\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()  \n",
    "    \n",
    "    loaded_model = model_from_json(loaded_model_json)  \n",
    "    \n",
    "    filename2 = IndividuPath + '/' + cnfigName  + 'current_structure.str'\n",
    "    specList = joblib.load(filename2)\n",
    "    \n",
    "    print('loadInvidualModel :IndividuPath=', IndividuPath)\n",
    "    \n",
    "    return loaded_model, specList\n",
    "\n",
    "def mutateIndividualModel(model, speclist):\n",
    "    \n",
    "    model1 = injectionModel(model, speclist, deltaspecList, params) \n",
    "    print('mutateIndividualModel :')\n",
    "    \n",
    "    return model1, speclist\n",
    "\n",
    "def copyModelFromInitial(originalModelPath, IndividuPath):\n",
    "    \n",
    "    shutil.copyfile(originalModelPath + 'current_structure.str', IndividuPath + 'current_structure.str')\n",
    " \n",
    "    \n",
    "\n",
    "def Idealpopulation(nstep):\n",
    "    \n",
    "    if (nstep < 4): \n",
    "        return 3 * nstep + 2\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injectionModel(model, modelspecList, deltaspecList, params): \n",
    "    \n",
    "    modelspeclist2 = [[modelspecList[i][0], modelspecList[i][1] + deltaspecList[i]] for i in range(len(modelspecList))]\n",
    "    model2 = buidModel(modelspeclist2, params)\n",
    "    \n",
    "    ## debut de la recopie des poids\n",
    "    weights1 = model.get_weights()\n",
    "    param_list = list(map(lambda k: k.shape, weights1))\n",
    "    weights2 = model2.get_weights()\n",
    "    \n",
    "  ## debut de la recopie des poids\n",
    "    for iparam in range(len(param_list)):\n",
    "        w = subcopy(weights1[iparam], weights2[iparam], param_list[iparam])\n",
    "        weights2[iparam] = w  \n",
    "    ########################################################################\n",
    "    ########################################################################\n",
    "    ########################################################################\n",
    "    #G = 4\n",
    "    #model2 = keras.utils.multi_gpu_model(model2, gpus = G)\n",
    "    #model2.compile(loss = 'mse', optimizer = optimizer, metrics = [\"mse\"])\n",
    "    model2.set_weights(weights2) \n",
    "    \n",
    "    return model2\n",
    "\n",
    "def mutateIndividualModel(model, specList, params):\n",
    "    \n",
    "    NbLayers = len(specList)\n",
    "    deltaNbNeuronList = BulleListPar(0, NbLayers)\n",
    "    \n",
    "    print('mutateIndividualModel : adding neurons', deltaNbNeuronList)\n",
    "    print('specList=', specList)\n",
    "    print('deltaNbNeuronList=', deltaNbNeuronList)\n",
    "    \n",
    "    model2 = injectionModel(model, specList, deltaNbNeuronList, params) \n",
    "    specList2 = [[specList[i][0], specList[i][1] + deltaNbNeuronList[i]] for i in range(NbLayers)]\n",
    "    \n",
    "    return model2, specList2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OP = {'lr':0.001, 'momentum':0.01, 'decay':0.99, 'nesterov':False}\n",
    "# opt = Optimizer('SGD', OP)\n",
    "\n",
    "# joblib.dump(opt, 'C:/Users/Sergey/Desktop/Natixis/Optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, opt_type, params):\n",
    "        \n",
    "        assert type(params) is dict, \"params must be a dict\" \n",
    "        assert opt_type in {'SGD', 'Adam'}, \"this type of optimizer is not supported\" \n",
    "        \n",
    "        self.type = opt_type\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'momentum', 'decay', 'nesterov'}, 'Illegal name of parameter'\n",
    "        elif self.type == 'Adam':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'beta_1', 'beta_2', 'epsilon', 'decay', 'amsgrad'}, 'Illegal name of parameter' \n",
    "        \n",
    "        self.params = params\n",
    "    \n",
    "    @classmethod   \n",
    "    def from_file(self, address):\n",
    "        \n",
    "        load = joblib.load(address)\n",
    "        \n",
    "        return load\n",
    "\n",
    "    def create_instance(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            return SGD(**self.params)\n",
    "        elif self.type == 'Adam':\n",
    "            return Adam(**self.params)\n",
    "        \n",
    "    def save_to_individu(self, address):\n",
    "        \n",
    "        try:\n",
    "            joblib.dump(self, address)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    def introduce_mutation(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            pass\n",
    "        elif self.type == 'Adam':\n",
    "            pass\n",
    "        \n",
    "        r = np.random.rand()\n",
    "        \n",
    "        if r < 0.15:\n",
    "            self.params['lr'] *= 2.0\n",
    "        elif r < 0.55:\n",
    "            self.params['lr'] *= 1.0\n",
    "        else:\n",
    "            self.params['lr'] *= 0.5\n",
    "            \n",
    "            \n",
    "class Task:\n",
    "    def __init__(self, ModelPath, individuPath, cnfigName):\n",
    "        \n",
    "        self.ModelPath = ModelPath \n",
    "        self.IndividuPath = individuPath\n",
    "        self.cnfigName = cnfigName \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Best_Model:\n",
    "    def __init__(self, task):\n",
    "        \n",
    "        model, specList = loadInvidualModel(task.ModelPath, task.cnfigName)\n",
    "        model2, specList2 = mutateIndividualModel(model, specList, params)\n",
    "        \n",
    "        self.parent_model = model\n",
    "        self.parent_speclist = specList\n",
    "        self.parent_weights = model.get_weights()\n",
    "        \n",
    "        self.model = model2\n",
    "        self.speclist = specList2\n",
    "        self.init_weights = model2.get_weights()\n",
    "        \n",
    "        with open(task.ModelPath + '/Best_score.txt', 'r+') as fp:\n",
    "            parent_score = float(fp.read())\n",
    "            \n",
    "        self.parent_score = parent_score\n",
    "        \n",
    "        self.task = task\n",
    "\n",
    "        #_________________updated values!_________________________________________\n",
    "        self.best_weights = model2.get_weights()\n",
    "        \n",
    "        # important!\n",
    "        self.best_score = parent_score\n",
    "\n",
    "        self.best_optimizer = None\n",
    "        self.epoch = 'Not Updated'\n",
    "        \n",
    "class CheckScoreSubmodels(tf.keras.callbacks.Callback):\n",
    "\n",
    "\n",
    "    def __init__(self, evaluate_model, best_weights, optimizer):\n",
    "        assert(type(best_weights[0]) is Best_Model)\n",
    "        \n",
    "        self.best_weights = best_weights\n",
    "        self.e_model = evaluate_model\n",
    "        self.test_generator = copy.deepcopy(TEST_GENs[len(best_weights)])\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \n",
    "        \n",
    "        scores = self.e_model.evaluate_generator(self.test_generator)\n",
    "        print('EVALUATING SUBMODELS...', scores)\n",
    "        \n",
    "        assert(len(scores[1:]) == len(self.best_weights))\n",
    "        assert(np.allclose(scores[0], np.sum(scores[1:]), atol = 0.01))\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.best_weights)):\n",
    "            \n",
    "            if self.best_weights[i].best_score > scores[i]:\n",
    "                \n",
    "                self.best_weights[i].best_weights = self.best_weights[i].model.get_weights()\n",
    "                self.best_weights[i].best_optimizer =  self.optimizer\n",
    "                self.best_weights[i].epoch = epoch\n",
    "                self.best_weights[i].best_score = scores[i]\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metamodels_create_and_train(tasks, list_optimizers, limit_tasks_by_model = params.N_MODELS_LIMIT_SIMULTANEOUSLY):\n",
    "    \n",
    "    print('N_TASKS: ', len(tasks))\n",
    "    batch_tasks = [tasks[i * limit_tasks_by_model : (i + 1) * limit_tasks_by_model]\\\n",
    "                   for i in range(len(tasks) // limit_tasks_by_model + 1)]\n",
    "    \n",
    "    TOTAL_BEST = {}\n",
    "    \n",
    "    for batch_task in batch_tasks:\n",
    "        if len(batch_task) == 0:\n",
    "            break\n",
    "        best_models = [Best_Model(task) for task in batch_task]\n",
    "        \n",
    "        for opt in list_optimizers:\n",
    "            \n",
    "            optimizer = opt.create_instance()\n",
    "            \n",
    "            list_models = [bm.model for bm in best_models]\n",
    "            \n",
    "            metamodel, eval_model = create_metamodel(list_models, optimizer)\n",
    "            \n",
    "            callbacks = [CheckScoreSubmodels(eval_model, best_models, opt)]\n",
    "\n",
    "            train_generator = copy.deepcopy(TRAIN_GENs[len(list_models)])\n",
    "            val_generator = copy.deepcopy(VAL_GENs[len(list_models)])\n",
    "            \n",
    "            fit_params = {'verbose':1, 'epochs':2, 'shuffle':False, 'use_multiprocessing':True, 'workers':16}\n",
    "            \n",
    "            print('####################### FITTING METAMODEL FOR %s#############################'%opt.type)\n",
    "            history = metamodel.fit_generator(generator = train_generator, validation_data = val_generator,\\\n",
    "                                       callbacks = callbacks, **fit_params)\n",
    "            \n",
    "            for bm in best_models:\n",
    "                bm.model.set_weights(bm.init_weights)\n",
    "        \n",
    "        for bm in best_models:\n",
    "            \n",
    "            if not bm.best_optimizer:\n",
    "                \n",
    "                bm.parent_model.set_weights(bm.parent_weights)\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Report.txt', 'w+') as fp:\n",
    "                    fp.write('Not Evolved from ' + bm.task.ModelPath + '\\n Parent speclist: ' + str(bm.parent_speclist))\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Best_score.txt', 'w+') as fp:\n",
    "                    fp.write(str(bm.parent_score))\n",
    "                    \n",
    "                saveIndividualModel(bm.parent_model, bm.parent_speclist, bm.task.IndividuPath, bm.task.cnfigName)\n",
    "                \n",
    "                TOTAL_BEST[bm.task.IndividuPath] = bm.best_score\n",
    "                \n",
    "            else:\n",
    "                msg = 'Evolved from '+ bm.task.ModelPath + ': ' + str(bm.parent_score) + \\\n",
    "                                    '-------> ' + str(bm.best_score) + '\\n'\n",
    "                msg = msg + 'Mutation: ' + str(bm.parent_speclist) + '-------> ' + str(bm.speclist) + '\\n'\n",
    "                \n",
    "                msg = msg + 'Best optimizer: ' + bm.best_optimizer.type + ' ' + str(bm.best_optimizer.params) + '\\n'\n",
    "                \n",
    "                msg = msg + 'Best epoch: ' + str(bm.epoch)\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Report.txt', 'w+') as fp:\n",
    "                    fp.write(msg)\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Best_score.txt', 'w+') as fp:\n",
    "                    fp.write(str(bm.best_score))\n",
    "                    \n",
    "                bm.model.set_weights(bm.best_weights)\n",
    "                saveIndividualModel(bm.model, bm.speclist, bm.task.IndividuPath, bm.task.cnfigName)\n",
    "                \n",
    "                TOTAL_BEST[bm.task.IndividuPath] = bm.best_score\n",
    "        \n",
    "    return TOTAL_BEST\n",
    "\n",
    "def create_metamodel(list_models, optimizer):\n",
    "    \n",
    "    inp = Input(shape = (192,))\n",
    "    \n",
    "    outs_list = [model(inp) for model in list_models]\n",
    "    \n",
    "    concat = Lambda(lambda l: K.concatenate(l, axis = -1))\n",
    "    \n",
    "    outs = concat(outs_list)\n",
    "    \n",
    "    assert(outs.shape[-1] == len(list_models))\n",
    "    assert(K.ndim(outs) == 2)\n",
    "    \n",
    "    \n",
    "    metaModel = Model(inputs = inp, outputs = outs)\n",
    "    \n",
    "    evaluate_model = Model(inputs = inp, outputs = outs_list)\n",
    "    \n",
    "    namespace = evaluate_model.output_names\n",
    "    print(evaluate_model.output_names)\n",
    "    \n",
    "    G = 16\n",
    "    metaModel = keras.utils.multi_gpu_model(metaModel, gpus = G)\n",
    "    metaModel.compile(loss = 'mse', optimizer = optimizer)\n",
    "    \n",
    "    evaluate_model.compile(loss = {namespace[i] : 'mse' for i in range(len(list_models))}, optimizer = optimizer)\n",
    "    \n",
    "    return metaModel, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed,\\\n",
    "                         nbloop, cnfigName, params, save = True, generate_errors = False):\n",
    "    \n",
    "    originalModelPath = params.PATH + \"/\" + pkgNameOriginal   \n",
    "\n",
    "    \n",
    "    SimuPath = params.PATH + \"/\" + pkgNameBut \n",
    "    if not(os.path.isdir(SimuPath)):\n",
    "        os.makedirs(SimuPath, 0o777)\n",
    "        \n",
    "    SimuPath = SimuPath + '/loop'\n",
    "    \n",
    "    if (os.path.isdir(SimuPath)):\n",
    "        shutil.rmtree(SimuPath)\n",
    "        \n",
    "    os.makedirs(SimuPath, 0o777 ) \n",
    "    precedingIndividualPathList =[]\n",
    "    \n",
    "    for iTimeStep in range(nbloop):\n",
    "        TimePath = SimuPath + \"/\" + \"time\" + str(iTimeStep) \n",
    "        \n",
    "        if not(os.path.isdir(TimePath)):\n",
    "                os.makedirs(TimePath, 0o777)\n",
    "                print(\"created : \",TimePath)\n",
    "                \n",
    "        LIST_OF_TASKS = []        \n",
    "        \n",
    "        if (iTimeStep == 0) :\n",
    "            for individual in range(InitialNbIndividual):\n",
    "                IndividuPath = TimePath +  \"/individu\" + str(individual)\n",
    "                \n",
    "                if not(os.path.isdir(IndividuPath)):\n",
    "                        os.makedirs(IndividuPath, 0o777)\n",
    "                        print(\"created : \",IndividuPath)\n",
    "                        \n",
    "                task = Task(originalModelPath, IndividuPath,  cnfigName)\n",
    "                \n",
    "                LIST_OF_TASKS.append(task)\n",
    "\n",
    "                precedingIndividualPathList.append(IndividuPath)                     \n",
    "        else :\n",
    "            \n",
    "            invidualpathList = []\n",
    "            individuNumero = 0\n",
    "            \n",
    "            for individualpath in precedingIndividualPathList:                            \n",
    "                \n",
    "                for child in range(nbChildAllowed): \n",
    "                    \n",
    "                    childPath = pathCreatechild(TimePath, individualpath, individuNumero)\n",
    "\n",
    "                    individuNumero += 1\n",
    "                    \n",
    "                    task = Task(individualpath, childPath, cnfigName)\n",
    "                    \n",
    "                    LIST_OF_TASKS.append(task)\n",
    "\n",
    "                    invidualpathList.append(childPath)\n",
    "                precedingIndividualPathList = invidualpathList\n",
    "           \n",
    "        K.clear_session()\n",
    "        NOTES_INDIVIDUS = metamodels_create_and_train(LIST_OF_TASKS, list_optimizers,\\\n",
    "                                limit_tasks_by_model = params.N_MODELS_LIMIT_SIMULTANEOUSLY)\n",
    "\n",
    "        SORTED_INDIVIDUS = sorted(NOTES_INDIVIDUS, key = NOTES_INDIVIDUS.__getitem__)\n",
    "        \n",
    "        N_survive = populationctrlFunc(iTimeStep)\n",
    "        bestNoteIndividus = SORTED_INDIVIDUS[:N_survive]\n",
    "        worstNoteIndividus = SORTED_INDIVIDUS[N_survive:]\n",
    "        \n",
    "        print('ListOnote=', SORTED_INDIVIDUS)\n",
    "        print('bestNoteIndexlist=', bestNoteIndividus)\n",
    "        print('worstNoteIndexlist=', worstNoteIndividus)\n",
    "        \n",
    "        for individu in worstNoteIndividus : \n",
    "            killIndividual(individu)\n",
    "            \n",
    "        precedingIndividualPathList = [individu for individu in bestNoteIndividus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created :  .//New_Simu_Test/loop/time0\n",
      "created :  .//New_Simu_Test/loop/time0/individu0\n",
      "created :  .//New_Simu_Test/loop/time0/individu1\n",
      "created :  .//New_Simu_Test/loop/time0/individu2\n",
      "created :  .//New_Simu_Test/loop/time0/individu3\n",
      "N_TASKS:  4\n",
      "loadInvidualModel :IndividuPath= .//New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= .//New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= .//New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= .//New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 192 and 193 for 'model_1/Layer0/MatMul' (op: 'MatMul') with input shapes: [?,192], [193,23].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1864\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 192 and 193 for 'model_1/Layer0/MatMul' (op: 'MatMul') with input shapes: [?,192], [193,23].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f51ed61691e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m a = ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut,\\\n\u001b[0;32m---> 15\u001b[0;31m                     InitialNbIndividual, Idealpopulation, nbChildAllowed, nbloop, \"Vol\", params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-1ec56ffdbef7>\u001b[0m in \u001b[0;36mReinforceOptimalityWithGenetic\u001b[0;34m(list_optimizers, pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed, nbloop, cnfigName, params, save, generate_errors)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         NOTES_INDIVIDUS = metamodels_create_and_train(LIST_OF_TASKS, list_optimizers,\\\n\u001b[0;32m---> 63\u001b[0;31m                                 limit_tasks_by_model = params.N_MODELS_LIMIT_SIMULTANEOUSLY)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mSORTED_INDIVIDUS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNOTES_INDIVIDUS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOTES_INDIVIDUS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-37ceb7aebf6b>\u001b[0m in \u001b[0;36mmetamodels_create_and_train\u001b[0;34m(tasks, list_optimizers, limit_tasks_by_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mlist_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmetamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_metamodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCheckScoreSubmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-37ceb7aebf6b>\u001b[0m in \u001b[0;36mcreate_metamodel\u001b[0;34m(list_models, optimizer)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mouts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-37ceb7aebf6b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mouts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m    719\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                             output_tensors = to_list(\n\u001b[0;32m--> 721\u001b[0;31m                                 layer.call(computed_tensor, **kwargs))\n\u001b[0m\u001b[1;32m    722\u001b[0m                             output_masks = layer.compute_mask(computed_tensor,\n\u001b[1;32m    723\u001b[0m                                                               computed_mask)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2647\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5923\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5924\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5925\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5926\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 2027\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 192 and 193 for 'model_1/Layer0/MatMul' (op: 'MatMul') with input shapes: [?,192], [193,23]."
     ]
    }
   ],
   "source": [
    "nbloop = 5\n",
    "\n",
    "nbChildAllowed = 4\n",
    "pkgNameOriginal = params.LEARNINGBASE_ORIGIN\n",
    "pkgNameBut = params.LEARNINGBASE_BUT\n",
    "InitialNbIndividual = 4\n",
    "naturalSelectionPercentage = 0.5\n",
    "\n",
    "\n",
    "list_opt = [('Adam', {'lr':0.0001}), ('SGD', {'lr':0.0001})]\n",
    "list_optimizers = [Optimizer(lopt[0], lopt[1]) for lopt in list_opt]\n",
    "\n",
    "\n",
    "a = ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut,\\\n",
    "                    InitialNbIndividual, Idealpopulation, nbChildAllowed, nbloop, \"Vol\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(keras)\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"asdasd\".find('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import horovod as hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
