{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1: How to take advantage of multiple availible GPUs?\n",
    "\n",
    "\n",
    "- ### Suggested solution\n",
    "\n",
    "For every trained model we add the following line(for 16 GPUs as __in the case of DGX-2__):\n",
    "\n",
    "model = keras.utils.multi_gpu_model(model, gpus = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #2: How to source training batches from several ''heavy'' files? \n",
    "\n",
    "- ### Suggested solution\n",
    "\n",
    "We use a DataGenerator in the following way:\n",
    "\n",
    "- Suppose that we have 4 batches of 64K instances __by file__ (containing 256K instances). We need to open this file only once instead of 4 times for every single batch inside. We create a generator as shown in the cell below. We know that for example __file_1__ contains batches 0 to 3 and __file_2__ contains files 4 to 7. So in order to get the batch 4 we open __file_2__ and stock it in memory for batches 5,6 and 7. \n",
    "\n",
    "- We create a DataGenerator __once__ (train_generator) and then we recopy it for every new training using the following script:\n",
    "__copy.deepcopy(train_generator)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_N(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_FILES, size_FILES = 256000, batch_size = 64000, shuffle = True):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.list_FILES = list_FILES\n",
    "        \n",
    "        self.n_files = len(self.list_FILES)\n",
    "        \n",
    "        self.size_files = size_FILES\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "                                     \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.current_file = 0\n",
    "        \n",
    "        # We open the first file in the list\n",
    "        self.data = pd.read_csv(self.list_FILES[self.current_files], sep = ';')\n",
    "        \n",
    "        self.batches_per_files = size_FILES // batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_fichiers * self.size_fichiers / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        \n",
    "        # If the new batch is in another file, we update self.data \n",
    "        # by opening the next file in the row\n",
    "        if index // self.batches_per_fichier > self.current_fichier:\n",
    "            self.current_fichier += 1\n",
    "            self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "            self.data = self.data.reset_index(drop = 'index')\n",
    "        \n",
    "        intra_index = index % self.batches_per_fichier\n",
    "\n",
    "        data_temp = self.data.loc[intra_index * self.batch_size : (intra_index + 1) * self.batch_size - 1]\n",
    "        \n",
    "        Y = data_temp.price.values\n",
    "        X = data_temp.drop(columns = ['nbDates', 'price']).values\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Shuffles the list of files after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_FICHIERS)\n",
    "            \n",
    "        self.current_fichier = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #3: How to train several models in a parallel way?\n",
    "\n",
    "- ### Suggested solution\n",
    "\n",
    "Suppose we have a list of tasks where each task contains a __model to train__. \n",
    "\n",
    "For each task we create a new thread (__threading.Thread(target = procedure, args = (task)__). Each thread executes the following procedure:\n",
    "\n",
    "__________________________________________\n",
    "\n",
    "def procedure(task):\n",
    "\n",
    "    session = tf.Session()\n",
    "    K.set_session(session)\n",
    "    with session.as_default():\n",
    "        with session.graph.as_default():\n",
    "            task.model.fit_generator(...)\n",
    "__________________________________________\n",
    "\n",
    "- ##### Will the line __K.set_session(session)__ executed by many threads in parallel __for different sessions__ provoke an interaction between sessions?\n",
    "\n",
    "- ##### Is it a good approach to training several models in parallel?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #4: How to avoid memory chunk in Tensorflow? Can the procedure described in Question #3 affect it?\n",
    "\n",
    "- ### Suggested solution\n",
    "\n",
    "We clear sessions and Graphs inside the __procedure(task)__ as follows (by every single Thread):\n",
    "\n",
    "\n",
    "__________________________________________\n",
    "\n",
    "def procedure(task):\n",
    "\n",
    "    session = tf.Session()\n",
    "    K.set_session(session)\n",
    "    \n",
    "    model = task.get_model()\n",
    "    \n",
    "    with session.as_default():\n",
    "        with session.graph.as_default():\n",
    "            model.fit_generator(...)\n",
    "    \n",
    "    # clearing the session and resetting it's graph\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    gc.collect()\n",
    "    \n",
    "    # closing the session\n",
    "    session.close()        \n",
    "     \n",
    "__________________________________________\n",
    "\n",
    "- ##### Is this redundant?\n",
    "- ##### Will this do what we expect?\n",
    "- ##### Will this help to a avoid memory chunk ?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
