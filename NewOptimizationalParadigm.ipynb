{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "\n",
    "from sys import platform\n",
    "from decimal import *\n",
    "\n",
    "\n",
    "path='./'\n",
    "if platform == 'win32' :\n",
    "    #path ='/workspace'\n",
    "    path ='C:/Users/Sergey/Desktop/Natixis/Yeti'\n",
    "\n",
    "\n",
    "import keras, tensorflow, pkg_resources\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from math import sqrt, exp, log, erf,floor\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "import keras.layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from IPython.display import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import threading\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "# Generate dummy data\n",
    "\n",
    "\n",
    "\n",
    "getcontext().prec = 8\n",
    "from util_functions_YetiPhen_VolLoc import *\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "ListField1 ={\"S1\", \"mu1\", \"bonus\", \"YetiBarrier\", \"YetiCoupon\", \"PhoenixBarrier\",\"PhoenixCoupon\",\"PDIBarrier\",\"PDIGearing\",\"PDIStrike\",\"PDIType\",\n",
    "             \"maturity\",\"nbDates\"}\n",
    "\n",
    "ListField2={\"vol-date0-strike0\",\"vol-date0-strike1\",\"vol-date0-strike2\",\"vol-date0-strike3\",\"vol-date0-strike4\",\"vol-date0-strike5\",\"vol-date0-strike6\",\n",
    "           \"vol-date0-strike7\",\"vol-date0-strike8\",\"vol-date0-strike9\",\"vol-date0-strike10\",\"vol-date0-strike11\",\"vol-date0-strike12\",\"vol-date0-strike13\",\n",
    "           \"vol-date0-strike14\",\"vol-date1-strike0\",\"vol-date1-strike1\",\"vol-date1-strike2\",\"vol-date1-strike3\",\"vol-date1-strike4\",\"vol-date1-strike5\",\n",
    "           \"vol-date1-strike6\",\"vol-date1-strike7\",\"vol-date1-strike8\",\"vol-date1-strike9\",\"vol-date1-strike10\",\"vol-date1-strike11\",\"vol-date1-strike12\",\n",
    "           \"vol-date1-strike13\",\"vol-date1-strike14\",\"vol-date2-strike0\",\"vol-date2-strike1\",\"vol-date2-strike2\",\"vol-date2-strike3\",\"vol-date2-strike4\",\n",
    "           \"vol-date2-strike5\",\"vol-date2-strike6\",\"vol-date2-strike7\",\"vol-date2-strike8\",\"vol-date2-strike9\",\"vol-date2-strike10\",\"vol-date2-strike11\",\n",
    "           \"vol-date2-strike12\",\"vol-date2-strike13\",\"vol-date2-strike14\",\"vol-date3-strike0\",\"vol-date3-strike1\",\"vol-date3-strike2\",\"vol-date3-strike3\",\n",
    "           \"vol-date3-strike4\",\"vol-date3-strike5\",\"vol-date3-strike6\",\"vol-date3-strike7\",\"vol-date3-strike8\",\"vol-date3-strike9\",\"vol-date3-strike10\",\n",
    "           \"vol-date3-strike11\",\"vol-date3-strike12\",\"vol-date3-strike13\",\"vol-date3-strike14\",\"vol-date4-strike0\",\"vol-date4-strike1\",\"vol-date4-strike2\",\n",
    "           \"vol-date4-strike3\",\"vol-date4-strike4\",\"vol-date4-strike5\",\"vol-date4-strike6\",\"vol-date4-strike7\",\"vol-date4-strike8\",\"vol-date4-strike9\",\n",
    "           \"vol-date4-strike10\",\"vol-date4-strike11\",\"vol-date4-strike12\",\"vol-date4-strike13\",\"vol-date4-strike14\",\"vol-date5-strike0\",\"vol-date5-strike1\",\n",
    "           \"vol-date5-strike2\",\"vol-date5-strike3\",\"vol-date5-strike4\",\"vol-date5-strike5\",\"vol-date5-strike6\",\"vol-date5-strike7\",\"vol-date5-strike8\",\n",
    "           \"vol-date5-strike9\",\"vol-date5-strike10\",\"vol-date5-strike11\",\"vol-date5-strike12\",\"vol-date5-strike13\",\"vol-date5-strike14\",\"vol-date6-strike0\",\n",
    "           \"vol-date6-strike1\",\"vol-date6-strike2\",\"vol-date6-strike3\",\"vol-date6-strike4\",\"vol-date6-strike5\",\"vol-date6-strike6\",\"vol-date6-strike7\",\n",
    "           \"vol-date6-strike8\",\"vol-date6-strike9\",\"vol-date6-strike10\",\"vol-date6-strike11\",\"vol-date6-strike12\",\"vol-date6-strike13\",\"vol-date6-strike14\",\n",
    "           \"vol-date7-strike0\",\"vol-date7-strike1\",\"vol-date7-strike2\",\"vol-date7-strike3\",\"vol-date7-strike4\",\"vol-date7-strike5\",\"vol-date7-strike6\",\n",
    "           \"vol-date7-strike7\",\"vol-date7-strike8\",\"vol-date7-strike9\",\"vol-date7-strike10\",\"vol-date7-strike11\",\"vol-date7-strike12\",\"vol-date7-strike13\",\n",
    "           \"vol-date7-strike14\",\"vol-date8-strike0\",\"vol-date8-strike1\",\"vol-date8-strike2\",\"vol-date8-strike3\",\"vol-date8-strike4\",\"vol-date8-strike5\",\n",
    "           \"vol-date8-strike6\",\"vol-date8-strike7\",\"vol-date8-strike8\",\"vol-date8-strike9\",\"vol-date8-strike10\",\"vol-date8-strike11\",\"vol-date8-strike12\",\n",
    "           \"vol-date8-strike13\",\"vol-date8-strike14\",\"vol-date9-strike0\",\"vol-date9-strike1\",\"vol-date9-strike2\",\"vol-date9-strike3\",\"vol-date9-strike4\",\n",
    "           \"vol-date9-strike5\",\"vol-date9-strike6\",\"vol-date9-strike7\",\"vol-date9-strike8\",\"vol-date9-strike9\",\"vol-date9-strike10\",\"vol-date9-strike11\",\n",
    "           \"vol-date9-strike12\",\"vol-date9-strike13\",\"vol-date9-strike14\",\"vol-date10-strike0\",\"vol-date10-strike1\",\"vol-date10-strike2\",\"vol-date10-strike3\",\n",
    "           \"vol-date10-strike4\",\"vol-date10-strike5\",\"vol-date10-strike6\",\"vol-date10-strike7\",\"vol-date10-strike8\",\"vol-date10-strike9\",\"vol-date10-strike10\",\n",
    "           \"vol-date10-strike11\",\"vol-date10-strike12\",\"vol-date10-strike13\",\"vol-date10-strike14\",\"vol-date11-strike0\",\"vol-date11-strike1\",\"vol-date11-strike2\",\n",
    "           \"vol-date11-strike3\",\"vol-date11-strike4\",\"vol-date11-strike5\",\"vol-date11-strike6\",\"vol-date11-strike7\",\n",
    "           \"vol-date11-strike8\",\"vol-date11-strike9\",\"vol-date11-strike10\",\"vol-date11-strike11\",\"vol-date11-strike12\",\"vol-date11-strike13\",\"vol-date11-strike14\"}\n",
    "\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "params = metaparameters()\n",
    "\n",
    "params.INPUT_DIM  = 193\n",
    "params.INPUT_OPTION = 193\n",
    "params.INPUT_VOL = 13\n",
    "params.NB_NEURON_PRINCIPAL = 8\n",
    "params.ACTIVATION_PRINCIPALE = 'tanh'\n",
    "params.ACTIVATION_PRINCIPALE_FINALE = 'linear'\n",
    "params.INITIAL_LEARNING_NB_EPOCH = 1000\n",
    "params.LEARNINGBASE_ORIGIN = \"New_Test\"\n",
    "params.LEARNINGBASE_BUT = \"New_Simu_Test\"\n",
    "params.GENETIC_LEARNING_NB_EPOCH = 10\n",
    "params.BATCH_SIZE_PRINCIPAL = 32768\n",
    "params.OPTIMIZER = 'adamax'##############################\n",
    "params.OPTIMIZER_GENETIC = 'SGD'###########################\n",
    "params.NBLAYERS = 11\n",
    "params.NB_LOOPS = 5\n",
    "params.PATH = path\n",
    "params.VERBOSE_FLAG = 2\n",
    "params.EPSILON_GREEDINESS = 0.25\n",
    "params.EPSILON_GREEDINESS_DECREASING_FACTOR = 0.99\n",
    "params.INITIAL_NETWORK_STRUCTURE = [[0,10], [1,20], [2,30], [0,10]]\n",
    "params.NOTATION_FILE = dataDirectory + '/' + dataNotationFile\n",
    "params.LISTFIELD1 = ListField1\n",
    "params.LISTFIELD2 = ListField2\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps de lecture du ficher 0.0740351676940918\n",
      "temps de lecture du ficher 0.06249523162841797\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "dataframe = pandas.read_csv(dataDirectory + \"/\" + dataLearningFile)\n",
    "endTime = time.time()\n",
    "print('temps de lecture du ficher', endTime - startTime)\n",
    "X = dataframe.loc[:, ListField1.union(ListField2)];\n",
    "Y_Vol = dataframe['price'];\n",
    "datasize = X.size\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "X_scaled = (X_scaler.fit_transform(X))\n",
    "\n",
    "params.X_scaler = X_scaler\n",
    "params.X_scaled = X_scaled\n",
    "params.X = X\n",
    "Y_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "Y_scaled = (Y_scaler.fit_transform(np.array(Y_Vol).reshape(-1, 1))).reshape(1, -1)[0]\n",
    "params.Y_scaler = Y_scaler\n",
    "params.Y_scaled = Y_scaled\n",
    "params.Y_Vol = Y_Vol\n",
    "\n",
    "startTime = time.time()\n",
    "dataframe = pandas.read_csv(dataDirectory + \"/\" + dataNotationFile)\n",
    "endTime = time.time()\n",
    "print('temps de lecture du ficher', endTime - startTime)\n",
    "X = dataframe.loc[:, ListField1.union(ListField2)];\n",
    "Y_Vol = dataframe['price'];\n",
    "datasize = X.size\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "X_scaled = (X_scaler.fit_transform(X))\n",
    "params.X_Notation_scaler = X_scaler\n",
    "params.X_Notation_scaled = X_scaled\n",
    "params.X_Notation = X\n",
    "Y_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "Y_scaled = (Y_scaler.fit_transform(np.array(Y_Vol).reshape(-1, 1))).reshape(1, -1)[0]\n",
    "params.Y_Notation_scaler = Y_scaler\n",
    "params.Y_Notation_scaled = Y_scaled\n",
    "params.Y_Notation_Vol = Y_Vol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_N(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_FICHIERS, n_dims = 16, size_FICHIERS = 10000, batch_size = 20, shuffle=True, mode = 'train'):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.list_FICHIERS = list_FICHIERS\n",
    "        \n",
    "        self.n_fichiers = len(self.list_FICHIERS)\n",
    "        \n",
    "        self.size_fichiers = size_FICHIERS\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "               \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        assert(mode in {'train', 'val', 'test'})\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.current_fichier = 0\n",
    "        # '256Ks/'+ str(self.list_FICHIERS[self.current_fichier]) +'.csv'\n",
    "        self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "        \n",
    "        self.batches_per_fichier = size_FICHIERS // batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_fichiers * self.size_fichiers / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):                                    \n",
    "        if index // self.batches_per_fichier != self.current_fichier:\n",
    "            self.current_fichier = index // self.batches_per_fichier\n",
    "            # '256Ks/'+ str(self.list_FICHIERS[self.current_fichier]) +'.csv'\n",
    "            self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "            self.data = self.data.reset_index(drop = 'index')\n",
    "        \n",
    "        intra_index = index % self.batches_per_fichier\n",
    "\n",
    "        data_temp = self.data.loc[intra_index * self.batch_size : (intra_index + 1) * self.batch_size - 1]\n",
    "        \n",
    "        Y = data_temp.price.values\n",
    "        X = data_temp.drop(columns = ['nbDates', 'price']).values\n",
    "\n",
    "        \n",
    "        if self.mode in {'train', 'val'}:\n",
    "            Y = Y.reshape((-1, 1))\n",
    "            Y = np.squeeze(np.repeat(Y[:, np.newaxis, :], self.n_dims, axis = 1))\n",
    "        else:\n",
    "            Y = [Y for i in range(self.n_dims)]\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_FICHIERS)\n",
    "\n",
    "        self.current_fichier = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to = '/workspace/FirstAttempt/Data/Sergey/'\n",
    "path_to = 'C:/Users/Sergey/Desktop/Natixis/DataSamples/'\n",
    "list_gen = [path_to + str(i) + '.csv' for i in range(5)]\n",
    "\n",
    "TRAIN_GENs = {i : DataGenerator_N(list_gen[:3], n_dims = i, mode = 'train') for i in range(20)}\n",
    "VAL_GENs = {i : DataGenerator_N([list_gen[3]], n_dims = i, mode = 'val') for i in range(20)}\n",
    "TEST_GENs = {i : DataGenerator_N([list_gen[4]], n_dims = i, mode = 'test') for i in range(20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Val_GEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-866dddf79426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mVal_GEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Val_GEN' is not defined"
     ]
    }
   ],
   "source": [
    "Val_GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_GEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e6c6e6ccfc2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_GEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Train_GEN' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "copy.deepcopy(Train_GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Y_ordre1_Vol, model_Initial, y_scaler_Vol, y_scaled_Vol = InitialCalibration2(params.Y_Vol, \"Vol\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildParamList(modelspecList, params):\n",
    "    \n",
    "    model = buidModel(modelspecList, params)   \n",
    "    weights1 = model.get_weights()\n",
    "    \n",
    "    return list(map(lambda k: k.shape, weights1))\n",
    "\n",
    "def CountIndividuals(a):\n",
    "    \n",
    "    ii = 0\n",
    "    for f in os.listdir(a):\n",
    "        ii = ii + 1\n",
    "        \n",
    "    return ii\n",
    "\n",
    "def convergeIndividualModel(model2):\n",
    "    \n",
    "    print('convergeIndividualModel :')\n",
    "    \n",
    "    return None\n",
    "   \n",
    "\n",
    "def pathCreatechild(TimePath, individualpath, child):\n",
    "    \n",
    "    pathCreated = TimePath + '/individu' +  str(child)\n",
    "    \n",
    "    if not(os.path.isdir(pathCreated)):\n",
    "        os.makedirs(pathCreated, 0o777)\n",
    "        \n",
    "    print('pathCreatechild=', pathCreated)\n",
    "    \n",
    "    return pathCreated\n",
    "\n",
    "def killIndividual(individualPath):\n",
    "    \n",
    "    print(\"killing :\", individualPath)\n",
    "    shutil.rmtree(individualPath)\n",
    "    \n",
    "def saveIndividualModel(model, specList, path, cnfigName):\n",
    "    \n",
    "    model.save_weights(path + '/' + cnfigName + '.hdf5')\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(path + '/' + cnfigName +'.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    filename = path + '/' + cnfigName + 'current_structure.str'\n",
    "    _= joblib.dump(specList, filename, compress = 9)\n",
    "    \n",
    "    print('saveIndividualModel model2 at :', path)\n",
    "\n",
    "    \n",
    "def loadInvidualModel(IndividuPath, cnfigName):\n",
    "    \n",
    "    json_file = open(IndividuPath + \"/\" + cnfigName  + '.json', 'r')\n",
    "    file = IndividuPath + '/' + cnfigName  + '.json'\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()  \n",
    "    \n",
    "    loaded_model = model_from_json(loaded_model_json)  \n",
    "    \n",
    "    filename2 = IndividuPath + '/' + cnfigName  + 'current_structure.str'\n",
    "    specList = joblib.load(filename2)\n",
    "    \n",
    "    print('loadInvidualModel :IndividuPath=', IndividuPath)\n",
    "    \n",
    "    return loaded_model, specList\n",
    "\n",
    "def mutateIndividualModel(model, speclist):\n",
    "    \n",
    "    model1 = injectionModel(model, speclist, deltaspecList, params) \n",
    "    print('mutateIndividualModel :')\n",
    "    \n",
    "    return model1, speclist\n",
    "\n",
    "def copyModelFromInitial(originalModelPath, IndividuPath):\n",
    "    \n",
    "    shutil.copyfile(originalModelPath + 'current_structure.str', IndividuPath + 'current_structure.str')\n",
    " \n",
    "    \n",
    "\n",
    "def Idealpopulation(nstep):\n",
    "    \n",
    "    if (nstep < 4): \n",
    "        return 3 * nstep + 2\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injectionModel(model, modelspecList, deltaspecList, params): \n",
    "    \n",
    "    modelspeclist2 = [[modelspecList[i][0], modelspecList[i][1] + deltaspecList[i]] for i in range(len(modelspecList))]\n",
    "    model2 = buidModel(modelspeclist2, params)\n",
    "    \n",
    "    ## debut de la recopie des poids\n",
    "    weights1 = model.get_weights()\n",
    "    param_list = list(map(lambda k: k.shape, weights1))\n",
    "    weights2 = model2.get_weights()\n",
    "    \n",
    "  ## debut de la recopie des poids\n",
    "    for iparam in range(len(param_list)):\n",
    "        w = subcopy(weights1[iparam], weights2[iparam], param_list[iparam])\n",
    "        weights2[iparam] = w  \n",
    "    ########################################################################\n",
    "    ########################################################################\n",
    "    ########################################################################\n",
    "    #G = 4\n",
    "    #model2 = keras.utils.multi_gpu_model(model2, gpus = G)\n",
    "    #model2.compile(loss = 'mse', optimizer = optimizer, metrics = [\"mse\"])\n",
    "    model2.set_weights(weights2) \n",
    "    \n",
    "    return model2\n",
    "\n",
    "def mutateIndividualModel(model, specList, params):\n",
    "    \n",
    "    NbLayers = len(specList)\n",
    "    deltaNbNeuronList = BulleListPar(0, NbLayers)\n",
    "    \n",
    "    print('mutateIndividualModel : adding neurons', deltaNbNeuronList)\n",
    "    print('specList=', specList)\n",
    "    print('deltaNbNeuronList=', deltaNbNeuronList)\n",
    "    \n",
    "    model2 = injectionModel(model, specList, deltaNbNeuronList, params) \n",
    "    specList2 = [[specList[i][0], specList[i][1] + deltaNbNeuronList[i]] for i in range(NbLayers)]\n",
    "    \n",
    "    return model2, specList2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OP = {'lr':0.001, 'momentum':0.01, 'decay':0.99, 'nesterov':False}\n",
    "# opt = Optimizer('SGD', OP)\n",
    "\n",
    "# joblib.dump(opt, 'C:/Users/Sergey/Desktop/Natixis/Optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, opt_type, params):\n",
    "        \n",
    "        assert type(params) is dict, \"params must be a dict\" \n",
    "        assert opt_type in {'SGD', 'Adam'}, \"this type of optimizer is not supported\" \n",
    "        \n",
    "        self.type = opt_type\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'momentum', 'decay', 'nesterov'}, 'Illegal name of parameter'\n",
    "        elif self.type == 'Adam':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'beta_1', 'beta_2', 'epsilon', 'decay', 'amsgrad'}, 'Illegal name of parameter' \n",
    "        \n",
    "        self.params = params\n",
    "    \n",
    "    @classmethod   \n",
    "    def from_file(self, address):\n",
    "        \n",
    "        load = joblib.load(address)\n",
    "        \n",
    "        return load\n",
    "\n",
    "    def create_instance(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            return SGD(**self.params)\n",
    "        elif self.type == 'Adam':\n",
    "            return Adam(**self.params)\n",
    "        \n",
    "    def save_to_individu(self, address):\n",
    "        \n",
    "        try:\n",
    "            joblib.dump(self, address)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    def introduce_mutation(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            pass\n",
    "        elif self.type == 'Adam':\n",
    "            pass\n",
    "        \n",
    "        r = np.random.rand()\n",
    "        \n",
    "        if r < 0.15:\n",
    "            self.params['lr'] *= 2.0\n",
    "        elif r < 0.55:\n",
    "            self.params['lr'] *= 1.0\n",
    "        else:\n",
    "            self.params['lr'] *= 0.5\n",
    "            \n",
    "            \n",
    "class Task:\n",
    "    def __init__(self, ModelPath, individuPath, cnfigName):\n",
    "        \n",
    "        self.ModelPath = ModelPath \n",
    "        self.IndividuPath = individuPath\n",
    "        self.cnfigName = cnfigName \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Best_Model:\n",
    "    def __init__(self, task):\n",
    "        \n",
    "        model, specList = loadInvidualModel(task.ModelPath, task.cnfigName)\n",
    "        model2, specList2 = mutateIndividualModel(model, specList, params)\n",
    "        \n",
    "        self.parent_model = model\n",
    "        self.parent_speclist = specList\n",
    "        self.parent_weights = model.get_weights()\n",
    "        \n",
    "        self.model = model2\n",
    "        self.speclist = specList2\n",
    "        self.init_weights = model2.get_weights()\n",
    "        \n",
    "        with open(task.ModelPath + '/Best_score.txt', 'r+') as fp:\n",
    "            parent_score = float(fp.read())\n",
    "            \n",
    "        self.parent_score = parent_score\n",
    "        \n",
    "        self.task = task\n",
    "\n",
    "        #_________________updated values!_________________________________________\n",
    "        self.best_weights = model2.get_weights()\n",
    "        \n",
    "        # important!\n",
    "        self.best_score = parent_score\n",
    "\n",
    "        self.best_optimizer = None\n",
    "        self.epoch = 'Not Updated'\n",
    "        \n",
    "class CheckScoreSubmodels(tf.keras.callbacks.Callback):\n",
    "\n",
    "\n",
    "    def __init__(self, evaluate_model, best_weights, optimizer):\n",
    "        assert(type(best_weights[0]) is Best_Model)\n",
    "        \n",
    "        self.best_weights = best_weights\n",
    "        self.e_model = evaluate_model\n",
    "        self.test_generator = copy.deepcopy(TEST_GENs[len(best_weights)])\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \n",
    "        scores = self.e_model.evaluate_generator(self.test_generator)\n",
    "        print('EVALUATING SUBMODELS...', scores)\n",
    "        \n",
    "        assert(len(scores[1:]) == len(best_weights))\n",
    "        assert(np.allclose(scores[0], np.sum(scores[1:]), atol = 0.01))\n",
    "        \n",
    "        \n",
    "        for i in range(len(best_weights)):\n",
    "            \n",
    "            if self.best_weights[i].best_score > scores[i]:\n",
    "                \n",
    "                self.best_weights[i].best_weights = self.best_weights[i].model.get_weights()\n",
    "                self.best_weights[i].best_optimizer =  self.optimizer\n",
    "                self.best_weights[i].epoch = epoch\n",
    "                self.best_weights[i].best_score = scores[i]\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metamodels_create_and_train(tasks, list_optimizers, limit_tasks_by_model = 2):\n",
    "    \n",
    "    batch_tasks = [tasks[i * limit_tasks_by_model : (i + 1) * limit_tasks_by_model]\\\n",
    "                   for i in range(len(tasks) // limit_tasks_by_model + 1)]\n",
    "    \n",
    "    TOTAL_BEST = {}\n",
    "    \n",
    "    for batch_task in batch_tasks:\n",
    "        if len(batch_task) == 0:\n",
    "            break\n",
    "        best_models = [Best_Model(task) for task in batch_task]\n",
    "        \n",
    "        for opt in list_optimizers:\n",
    "            \n",
    "            optimizer = opt.create_instance()\n",
    "            \n",
    "            list_models = [bm.model for bm in best_models]\n",
    "            \n",
    "            metamodel, eval_model = create_metamodel(list_models, optimizer)\n",
    "            \n",
    "            callbacks = [CheckScoreSubmodels(eval_model, best_models, opt)]\n",
    "\n",
    "            train_generator = copy.deepcopy(TRAIN_GENs[len(list_models)])\n",
    "            val_generator = copy.deepcopy(VAL_GENs[len(list_models)])\n",
    "            \n",
    "            fit_params = {'verbose':1, 'epochs':2}\n",
    "            \n",
    "            print('####################### FITTING METAMODEL FOR %s#############################'%opt.type)\n",
    "            history = metamodel.fit_generator(generator = train_generator, validation_data = val_generator,\\\n",
    "                                       callbacks = callbacks, **fit_params)\n",
    "            \n",
    "            for bm in best_models:\n",
    "                bm.model.set_weights(bm.init_weights)\n",
    "        \n",
    "        for bm in best_models:\n",
    "            \n",
    "            if not bm.best_optimizer:\n",
    "                \n",
    "                bm.parent_model.set_weights(bm.parent_weights)\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Report.txt', 'w+') as fp:\n",
    "                    fp.write('Not Evolved from ' + bm.task.ModelPath + '\\n Parent speclist: ' + str(bm.parent_speclist))\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Best_score.txt', 'w+') as fp:\n",
    "                    fp.write(str(bm.parent_score))\n",
    "                    \n",
    "                saveIndividualModel(bm.parent_model, bm.parent_specList, bm.task.IndividuPath, bm.task.cnfigName)\n",
    "                \n",
    "                TOTAL_BEST[bm.task.IndividuPath] = bm.best_score\n",
    "                \n",
    "            else:\n",
    "                msg = 'Evolved from '+ bm.task.ModelPath + ': ' + str(bm.parent_weight) + \\\n",
    "                                    '-------> ' + str(bm.best_weight) + '\\n'\n",
    "                msg = msg + 'Mutation: ' + str(bm.parent_speclist) + '-------> ' + str(bm.speclist) + '\\n'\n",
    "                \n",
    "                msg = msg + 'Best optimizer:' + bm.best_optimizer.name + ' ' + str(bm.best_optimizer.params) + '\\n'\n",
    "                \n",
    "                msg = msg + 'Best epoch: ' + str(bm.best_epoch)\n",
    "                \n",
    "                with open(bm.task.IndividuPath + '/Report.txt', 'w+') as fp:\n",
    "                    fp.write(msg)\n",
    "                \n",
    "                with open(bm.task.ModelPath + '/Best_score.txt', 'w+') as fp:\n",
    "                    fp.write(str(bm.parent_score))\n",
    "                    \n",
    "                bm.model.set_weights(bm.best_weights)\n",
    "                saveIndividualModel(bm.model, bm.specList, bm.task.IndividuPath, bm.task.cnfigName)\n",
    "                \n",
    "                TOTAL_BEST[bm.task.IndividuPath] = bm.best_score\n",
    "        \n",
    "    return TOTAL_BEST\n",
    "\n",
    "def create_metamodel(list_models, optimizer):\n",
    "    \n",
    "    inp = Input(shape = (193,))\n",
    "    \n",
    "    for i, m in enumerate(list_models):\n",
    "        m.layers[-1].name = 'model_' + str(i + 1)\n",
    "    \n",
    "    namespace = [m.layers[-1].name for m in list_models]\n",
    "    print(namespace)\n",
    "    \n",
    "    outs_list = [model(inp) for model in list_models]\n",
    "    \n",
    "    concat = Lambda(lambda l: K.concatenate(l, axis = -1))\n",
    "    \n",
    "    outs = concat(outs_list)\n",
    "    \n",
    "    assert(outs.shape[-1] == len(list_models))\n",
    "    assert(K.ndim(outs) == 2)\n",
    "    \n",
    "    \n",
    "    metaModel = Model(inputs = inp, outputs = outs)\n",
    "    evaluate_model = Model(inputs = inp, outputs = outs_list)\n",
    "    \n",
    "    metaModel.compile(loss = 'mse', optimizer = optimizer)\n",
    "    \n",
    "    namespace = [m.layers[-1].name for m in list_models]\n",
    "    print(namespace)\n",
    "    \n",
    "    \n",
    "    evaluate_model.compile(loss = {namespace[i] : 'mse' for i in range(len(list_models))}, optimizer = optimizer)\n",
    "    \n",
    "    return metaModel, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed,\\\n",
    "                         nbloop, cnfigName, params, save = True, generate_errors = False):\n",
    "    \n",
    "    originalModelPath = params.PATH + \"/\" + pkgNameOriginal   \n",
    "\n",
    "    \n",
    "    SimuPath = params.PATH + \"/\" + pkgNameBut \n",
    "    if not(os.path.isdir(SimuPath)):\n",
    "        os.makedirs(SimuPath, 0o777)\n",
    "        \n",
    "    SimuPath = SimuPath + '/loop'\n",
    "    \n",
    "    if (os.path.isdir(SimuPath)):\n",
    "        shutil.rmtree(SimuPath)\n",
    "        \n",
    "    os.makedirs(SimuPath, 0o777 ) \n",
    "    precedingIndividualPathList =[]\n",
    "    \n",
    "    for iTimeStep in range(nbloop):\n",
    "        TimePath = SimuPath + \"/\" + \"time\" + str(iTimeStep) \n",
    "        \n",
    "        if not(os.path.isdir(TimePath)):\n",
    "                os.makedirs(TimePath, 0o777)\n",
    "                print(\"created : \",TimePath)\n",
    "                \n",
    "        LIST_OF_TASKS = []        \n",
    "        \n",
    "        if (iTimeStep == 0) :\n",
    "            for individual in range(InitialNbIndividual):\n",
    "                IndividuPath = TimePath +  \"/individu\" + str(individual)\n",
    "                \n",
    "                if not(os.path.isdir(IndividuPath)):\n",
    "                        os.makedirs(IndividuPath, 0o777)\n",
    "                        print(\"created : \",IndividuPath)\n",
    "                        \n",
    "                task = Task(originalModelPath, IndividuPath,  cnfigName)\n",
    "                \n",
    "                LIST_OF_TASKS.append(task)\n",
    "\n",
    "                precedingIndividualPathList.append(IndividuPath)                     \n",
    "        else :\n",
    "            \n",
    "            invidualpathList = []\n",
    "            individuNumero = 0\n",
    "            \n",
    "            for individualpath in precedingIndividualPathList:                            \n",
    "                \n",
    "                for child in range(nbChildAllowed): \n",
    "                    \n",
    "                    childPath = pathCreatechild(TimePath, individualpath, individuNumero)\n",
    "\n",
    "                    individuNumero += 1\n",
    "                    \n",
    "                    task = Task(individualpath, childPath, cnfigName)\n",
    "                    \n",
    "                    LIST_OF_TASKS.append(task)\n",
    "\n",
    "                    invidualpathList.append(childPath)\n",
    "                precedingIndividualPathList = invidualpathList\n",
    "           \n",
    "        K.clear_session()\n",
    "        NOTES_INDIVIDUS = metamodels_create_and_train(LIST_OF_TASKS, list_optimizers, limit_tasks_by_model = 4)\n",
    "\n",
    "        SORTED_INDIVIDUS = sorted(NOTES_INDIVIDUS, key = NOTES_INDIVIDUS.__getitem__)\n",
    "        \n",
    "        N_survive = populationctrlFunc(iTimeStep)\n",
    "        bestNoteIndividus = SORTED_INDIVIDUS[:N_survive]\n",
    "        worstNoteIndividus = SORTED_INDIVIDUS[N_survive:]\n",
    "        \n",
    "        print('ListOnote=', SORTED_INDIVIDUS)\n",
    "        print('bestNoteIndexlist=', bestNoteIndividus)\n",
    "        print('worstNoteIndexlist=', worstNoteIndividus)\n",
    "        \n",
    "        for individu in worstNoteIndividus : \n",
    "            killIndividual(individu)\n",
    "            \n",
    "        precedingIndividualPathList = [individu for individu in bestNoteIndividus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created :  C:/Users/Sergey/Desktop/Natixis/Yeti/New_Simu_Test/loop/time0\n",
      "created :  C:/Users/Sergey/Desktop/Natixis/Yeti/New_Simu_Test/loop/time0/individu0\n",
      "created :  C:/Users/Sergey/Desktop/Natixis/Yeti/New_Simu_Test/loop/time0/individu1\n",
      "created :  C:/Users/Sergey/Desktop/Natixis/Yeti/New_Simu_Test/loop/time0/individu2\n",
      "loadInvidualModel :IndividuPath= C:/Users/Sergey/Desktop/Natixis/Yeti/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= C:/Users/Sergey/Desktop/Natixis/Yeti/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= C:/Users/Sergey/Desktop/Natixis/Yeti/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "['model_1', 'model_2', 'model_3']\n",
      "['model_1', 'model_2', 'model_3']\n",
      "####################### FITTING METAMODEL FOR Adam#############################\n",
      "Epoch 1/2\n",
      " 162/1500 [==>...........................] - ETA: 8:47 - loss: 11.7468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9d52cb43c303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m a = ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut,\\\n\u001b[1;32m---> 16\u001b[1;33m                     InitialNbIndividual, Idealpopulation, nbChildAllowed, nbloop, \"Vol\", params)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9ef593341da5>\u001b[0m in \u001b[0;36mReinforceOptimalityWithGenetic\u001b[1;34m(list_optimizers, pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed, nbloop, cnfigName, params, save, generate_errors)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mNOTES_INDIVIDUS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetamodels_create_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLIST_OF_TASKS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_optimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_tasks_by_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mSORTED_INDIVIDUS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNOTES_INDIVIDUS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNOTES_INDIVIDUS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b595357e9ce4>\u001b[0m in \u001b[0;36mmetamodels_create_and_train\u001b[1;34m(tasks, list_optimizers, limit_tasks_by_model)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'####################### FITTING METAMODEL FOR %s#############################'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             history = metamodel.fit_generator(generator = train_generator, validation_data = val_generator,\\\n\u001b[1;32m---> 30\u001b[1;33m                                        callbacks = callbacks, **fit_params)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nbloop = 5\n",
    "NbIndividual = 3\n",
    "nbChildAllowed = 4\n",
    "\n",
    "pkgNameOriginal = params.LEARNINGBASE_ORIGIN\n",
    "pkgNameBut = params.LEARNINGBASE_BUT\n",
    "InitialNbIndividual = 3\n",
    "naturalSelectionPercentage = 0.5\n",
    "\n",
    "\n",
    "list_opt = [('Adam', {'lr':0.0001}), ('SGD', {'lr':0.0001})]\n",
    "list_optimizers = [Optimizer(lopt[0], lopt[1]) for lopt in list_opt]\n",
    "\n",
    "\n",
    "a = ReinforceOptimalityWithGenetic(list_optimizers, pkgNameOriginal, pkgNameBut,\\\n",
    "                    InitialNbIndividual, Idealpopulation, nbChildAllowed, nbloop, \"Vol\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(keras)\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
