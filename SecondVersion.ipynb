{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "\n",
    "from sys import platform\n",
    "from decimal import *\n",
    "\n",
    "\n",
    "path='./'\n",
    "if platform == 'win32' :\n",
    "    path ='C:/Users/Sergey/Desktop/Natixis/Yeti'\n",
    "    path_to = 'C:/Users/Sergey/Desktop/Natixis/DataSamples/'\n",
    "if platform == 'darwin' :\n",
    "    path ='/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc'   \n",
    "    path_to = '/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/Data/DataSamplesSerguey/'\n",
    "\n",
    "\n",
    "import keras, tensorflow, pkg_resources\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from math import sqrt, exp, log, erf,floor\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "import keras.layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from IPython.display import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import threading\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "# Generate dummy data\n",
    "\n",
    "\n",
    "\n",
    "getcontext().prec = 8\n",
    "from util_functions_YetiPhen_VolLoc import *\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "ListField1 ={\"S1\", \"mu1\", \"bonus\", \"YetiBarrier\", \"YetiCoupon\", \"PhoenixBarrier\",\"PhoenixCoupon\",\"PDIBarrier\",\"PDIGearing\",\"PDIStrike\",\"PDIType\",\n",
    "             \"maturity\",\"nbDates\"}\n",
    "\n",
    "ListField2={\"vol-date0-strike0\",\"vol-date0-strike1\",\"vol-date0-strike2\",\"vol-date0-strike3\",\"vol-date0-strike4\",\"vol-date0-strike5\",\"vol-date0-strike6\",\n",
    "           \"vol-date0-strike7\",\"vol-date0-strike8\",\"vol-date0-strike9\",\"vol-date0-strike10\",\"vol-date0-strike11\",\"vol-date0-strike12\",\"vol-date0-strike13\",\n",
    "           \"vol-date0-strike14\",\"vol-date1-strike0\",\"vol-date1-strike1\",\"vol-date1-strike2\",\"vol-date1-strike3\",\"vol-date1-strike4\",\"vol-date1-strike5\",\n",
    "           \"vol-date1-strike6\",\"vol-date1-strike7\",\"vol-date1-strike8\",\"vol-date1-strike9\",\"vol-date1-strike10\",\"vol-date1-strike11\",\"vol-date1-strike12\",\n",
    "           \"vol-date1-strike13\",\"vol-date1-strike14\",\"vol-date2-strike0\",\"vol-date2-strike1\",\"vol-date2-strike2\",\"vol-date2-strike3\",\"vol-date2-strike4\",\n",
    "           \"vol-date2-strike5\",\"vol-date2-strike6\",\"vol-date2-strike7\",\"vol-date2-strike8\",\"vol-date2-strike9\",\"vol-date2-strike10\",\"vol-date2-strike11\",\n",
    "           \"vol-date2-strike12\",\"vol-date2-strike13\",\"vol-date2-strike14\",\"vol-date3-strike0\",\"vol-date3-strike1\",\"vol-date3-strike2\",\"vol-date3-strike3\",\n",
    "           \"vol-date3-strike4\",\"vol-date3-strike5\",\"vol-date3-strike6\",\"vol-date3-strike7\",\"vol-date3-strike8\",\"vol-date3-strike9\",\"vol-date3-strike10\",\n",
    "           \"vol-date3-strike11\",\"vol-date3-strike12\",\"vol-date3-strike13\",\"vol-date3-strike14\",\"vol-date4-strike0\",\"vol-date4-strike1\",\"vol-date4-strike2\",\n",
    "           \"vol-date4-strike3\",\"vol-date4-strike4\",\"vol-date4-strike5\",\"vol-date4-strike6\",\"vol-date4-strike7\",\"vol-date4-strike8\",\"vol-date4-strike9\",\n",
    "           \"vol-date4-strike10\",\"vol-date4-strike11\",\"vol-date4-strike12\",\"vol-date4-strike13\",\"vol-date4-strike14\",\"vol-date5-strike0\",\"vol-date5-strike1\",\n",
    "           \"vol-date5-strike2\",\"vol-date5-strike3\",\"vol-date5-strike4\",\"vol-date5-strike5\",\"vol-date5-strike6\",\"vol-date5-strike7\",\"vol-date5-strike8\",\n",
    "           \"vol-date5-strike9\",\"vol-date5-strike10\",\"vol-date5-strike11\",\"vol-date5-strike12\",\"vol-date5-strike13\",\"vol-date5-strike14\",\"vol-date6-strike0\",\n",
    "           \"vol-date6-strike1\",\"vol-date6-strike2\",\"vol-date6-strike3\",\"vol-date6-strike4\",\"vol-date6-strike5\",\"vol-date6-strike6\",\"vol-date6-strike7\",\n",
    "           \"vol-date6-strike8\",\"vol-date6-strike9\",\"vol-date6-strike10\",\"vol-date6-strike11\",\"vol-date6-strike12\",\"vol-date6-strike13\",\"vol-date6-strike14\",\n",
    "           \"vol-date7-strike0\",\"vol-date7-strike1\",\"vol-date7-strike2\",\"vol-date7-strike3\",\"vol-date7-strike4\",\"vol-date7-strike5\",\"vol-date7-strike6\",\n",
    "           \"vol-date7-strike7\",\"vol-date7-strike8\",\"vol-date7-strike9\",\"vol-date7-strike10\",\"vol-date7-strike11\",\"vol-date7-strike12\",\"vol-date7-strike13\",\n",
    "           \"vol-date7-strike14\",\"vol-date8-strike0\",\"vol-date8-strike1\",\"vol-date8-strike2\",\"vol-date8-strike3\",\"vol-date8-strike4\",\"vol-date8-strike5\",\n",
    "           \"vol-date8-strike6\",\"vol-date8-strike7\",\"vol-date8-strike8\",\"vol-date8-strike9\",\"vol-date8-strike10\",\"vol-date8-strike11\",\"vol-date8-strike12\",\n",
    "           \"vol-date8-strike13\",\"vol-date8-strike14\",\"vol-date9-strike0\",\"vol-date9-strike1\",\"vol-date9-strike2\",\"vol-date9-strike3\",\"vol-date9-strike4\",\n",
    "           \"vol-date9-strike5\",\"vol-date9-strike6\",\"vol-date9-strike7\",\"vol-date9-strike8\",\"vol-date9-strike9\",\"vol-date9-strike10\",\"vol-date9-strike11\",\n",
    "           \"vol-date9-strike12\",\"vol-date9-strike13\",\"vol-date9-strike14\",\"vol-date10-strike0\",\"vol-date10-strike1\",\"vol-date10-strike2\",\"vol-date10-strike3\",\n",
    "           \"vol-date10-strike4\",\"vol-date10-strike5\",\"vol-date10-strike6\",\"vol-date10-strike7\",\"vol-date10-strike8\",\"vol-date10-strike9\",\"vol-date10-strike10\",\n",
    "           \"vol-date10-strike11\",\"vol-date10-strike12\",\"vol-date10-strike13\",\"vol-date10-strike14\",\"vol-date11-strike0\",\"vol-date11-strike1\",\"vol-date11-strike2\",\n",
    "           \"vol-date11-strike3\",\"vol-date11-strike4\",\"vol-date11-strike5\",\"vol-date11-strike6\",\"vol-date11-strike7\",\n",
    "           \"vol-date11-strike8\",\"vol-date11-strike9\",\"vol-date11-strike10\",\"vol-date11-strike11\",\"vol-date11-strike12\",\"vol-date11-strike13\",\"vol-date11-strike14\"}\n",
    "\n",
    "dataDirectory = 'Data'\n",
    "dataLearningFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "dataNotationFile = \"1dim_VolLoc-Example-new.CSV\"\n",
    "\n",
    "params = metaparameters()\n",
    "\n",
    "params.INPUT_DIM  = 193\n",
    "params.INPUT_OPTION = 193\n",
    "params.INPUT_VOL = 13\n",
    "params.NB_NEURON_PRINCIPAL = 8\n",
    "params.ACTIVATION_PRINCIPALE = 'tanh'\n",
    "params.ACTIVATION_PRINCIPALE_FINALE = 'linear'\n",
    "params.INITIAL_LEARNING_NB_EPOCH = 1000\n",
    "params.LEARNINGBASE_ORIGIN = \"New_Test\"\n",
    "params.LEARNINGBASE_BUT = \"New_Simu_Test\"\n",
    "params.GENETIC_LEARNING_NB_EPOCH = 10\n",
    "params.BATCH_SIZE_PRINCIPAL = 32768\n",
    "params.OPTIMIZER = 'adamax'##############################\n",
    "params.OPTIMIZER_GENETIC = 'SGD'###########################\n",
    "params.NBLAYERS = 11\n",
    "params.NB_LOOPS = 5\n",
    "params.PATH = path\n",
    "params.VERBOSE_FLAG = 2\n",
    "params.EPSILON_GREEDINESS = 0.25\n",
    "params.EPSILON_GREEDINESS_DECREASING_FACTOR = 0.99\n",
    "params.INITIAL_NETWORK_STRUCTURE = [[0,10], [1,20], [2,30], [0,10]]\n",
    "params.NOTATION_FILE = dataDirectory + '/' + dataNotationFile\n",
    "params.LISTFIELD1 = ListField1\n",
    "params.LISTFIELD2 = ListField2\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps de lecture du ficher 0.043168067932128906\n",
      "temps de lecture du ficher 0.040971994400024414\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "dataframe = pandas.read_csv(dataDirectory + \"/\" + dataLearningFile)\n",
    "endTime = time.time()\n",
    "print('temps de lecture du ficher', endTime - startTime)\n",
    "X = dataframe[ListField1.union(ListField2)];\n",
    "Y_Vol = dataframe['price'];\n",
    "datasize = X.size\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "X_scaled = (X_scaler.fit_transform(X))\n",
    "\n",
    "params.X_scaler = X_scaler\n",
    "params.X_scaled = X_scaled\n",
    "params.X = X\n",
    "Y_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "Y_scaled = (Y_scaler.fit_transform(np.array(Y_Vol).reshape(-1, 1))).reshape(1, -1)[0]\n",
    "params.Y_scaler = Y_scaler\n",
    "params.Y_scaled = Y_scaled\n",
    "params.Y_Vol = Y_Vol\n",
    "\n",
    "startTime = time.time()\n",
    "dataframe = pandas.read_csv(dataDirectory + \"/\" + dataNotationFile)\n",
    "endTime = time.time()\n",
    "print('temps de lecture du ficher', endTime - startTime)\n",
    "X = dataframe[ListField1.union(ListField2)];\n",
    "Y_Vol = dataframe['price'];\n",
    "datasize = X.size\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "X_scaled = (X_scaler.fit_transform(X))\n",
    "params.X_Notation_scaler = X_scaler\n",
    "params.X_Notation_scaled = X_scaled\n",
    "params.X_Notation = X\n",
    "Y_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))\n",
    "Y_scaled = (Y_scaler.fit_transform(np.array(Y_Vol).reshape(-1, 1))).reshape(1, -1)[0]\n",
    "params.Y_Notation_scaler = Y_scaler\n",
    "params.Y_Notation_scaled = Y_scaled\n",
    "params.Y_Notation_Vol = Y_Vol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_N(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_FICHIERS, size_FICHIERS = 10000, batch_size = 20, shuffle=True, mode = 'train'):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.list_FICHIERS = list_FICHIERS\n",
    "        \n",
    "        self.n_fichiers = len(self.list_FICHIERS)\n",
    "        \n",
    "        self.size_fichiers = size_FICHIERS\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "                                     \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.current_fichier = 0\n",
    "        # '256Ks/'+ str(self.list_FICHIERS[self.current_fichier]) +'.csv'\n",
    "        self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "        \n",
    "        self.batches_per_fichier = size_FICHIERS // batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_fichiers * self.size_fichiers / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        \n",
    "        if index // self.batches_per_fichier > self.current_fichier:\n",
    "            self.current_fichier += 1\n",
    "            # '256Ks/'+ str(self.list_FICHIERS[self.current_fichier]) +'.csv'\n",
    "            self.data = pd.read_csv(self.list_FICHIERS[self.current_fichier], sep = ';')\n",
    "            self.data = self.data.reset_index(drop = 'index')\n",
    "        \n",
    "        intra_index = index % self.batches_per_fichier\n",
    "\n",
    "        data_temp = self.data.loc[intra_index * self.batch_size : (intra_index + 1) * self.batch_size - 1]\n",
    "        \n",
    "        Y = data_temp.price.values\n",
    "        X = data_temp.drop(columns = ['nbDates', 'price']).values\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_FICHIERS)\n",
    "            \n",
    "        self.current_fichier = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_gen = [path_to + str(i) + '.csv' for i in range(5)]\n",
    "\n",
    "Train_GEN = DataGenerator_N(list_gen[:3])\n",
    "Val_GEN = DataGenerator_N(list_gen[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataGenerator_N at 0x7fa26d6c8e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "copy.deepcopy(Train_GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Y_ordre1_Vol, model_Initial, y_scaler_Vol, y_scaled_Vol = InitialCalibration2(params.Y_Vol, \"Vol\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildParamList(modelspecList, params) : \n",
    "    model = buidModel(modelspecList, params)   \n",
    "    weights1 = model.get_weights()\n",
    "    return list(map(lambda k: k.shape, weights1))\n",
    "\n",
    "def CountIndividuals(a)    :\n",
    "    ii = 0\n",
    "    for f in os.listdir(a):\n",
    "        ii = ii + 1\n",
    "    return ii\n",
    "\n",
    "def convergeIndividualModel(model2):\n",
    "    print('convergeIndividualModel :')\n",
    "    return None\n",
    "   \n",
    "\n",
    "def ComputeBestNoteIndexlist(ListOfnote, populationctrlFunc, iTimeStep):\n",
    "    lenlist = len(ListOfnote)\n",
    "    sortedlist = np.argsort(ListOfnote)\n",
    "    lim = populationctrlFunc(iTimeStep)\n",
    "    lim = min(lenlist, lim)\n",
    "    print('lim=', lim)\n",
    "    ListOfWorst = sortedlist[lim:] \n",
    "    ListOfBest = sortedlist[:lim]\n",
    "    return ListOfWorst, ListOfBest\n",
    "\n",
    "def pathCreatechild(TimePath, individualpath, child):\n",
    "    pathCreated = TimePath + '/individu' +  str(child)\n",
    "    if not(os.path.isdir(pathCreated)):\n",
    "        os.makedirs(pathCreated, 0o777)\n",
    "    print('pathCreatechild=', pathCreated)\n",
    "    return pathCreated\n",
    "\n",
    "def killIndividual(individualPath):\n",
    "    print(\"killing :\", individualPath)\n",
    "    shutil.rmtree(individualPath)\n",
    "    \n",
    "def saveIndividualModel(model, X_scaler, y_scaler, y_scaled, Y_ordre1, specList, path, cnfigName):\n",
    "    model.save_weights(path + '/' + cnfigName + '.hdf5')\n",
    "    # save model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(path + '/' + cnfigName +'.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # Save scaler Xand Y\n",
    "    filename = path + '/' + cnfigName + 'scalerX.pkl'\n",
    "    _= joblib.dump(X_scaler, filename, compress = 9)\n",
    "    filename = path + '/' + cnfigName + 'scalerY.pkl'\n",
    "    _= joblib.dump(y_scaler, filename, compress = 9)\n",
    "    filename = path + '/' + cnfigName + 'scaledY.pkl'\n",
    "    _= joblib.dump(y_scaled, filename, compress = 9)\n",
    "    filename = path + '/' + cnfigName + 'Y_ordre1.res'\n",
    "    _= joblib.dump(Y_ordre1, filename, compress = 9)\n",
    "    filename = path + '/' + cnfigName + 'current_structure.str'\n",
    "    _= joblib.dump(specList, filename, compress = 9)\n",
    "    print('saveIndividualModel model2 at :', path)\n",
    "\n",
    "    \n",
    "def loadInvidualModel(IndividuPath, cnfigName):\n",
    "    json_file = open(IndividuPath + \"/\" + cnfigName  + '.json', 'r')\n",
    "    file = IndividuPath + '/' + cnfigName  + '.json'\n",
    "    \n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()  \n",
    "    loaded_model = model_from_json(loaded_model_json)           \n",
    "    filename2 = IndividuPath + '/' + cnfigName  + 'current_structure.str'\n",
    "    specList = joblib.load(filename2 )\n",
    "    filename2 = IndividuPath + '/' + cnfigName  + 'scalerY.pkl'\n",
    "    y_scaler = joblib.load(filename2)\n",
    "    filename2 = IndividuPath + '/' + cnfigName  + 'scaledY.pkl'\n",
    "    y_scaled = joblib.load(filename2)\n",
    "    print('loadInvidualModel :IndividuPath=', IndividuPath)\n",
    "    return loaded_model, specList, y_scaler, y_scaled\n",
    "\n",
    "def mutateIndividualModel(model, speclist):\n",
    "    # specList = [[specList[i][0],specList[i][1]+  deltaNbNeuronList[i] ] for i in range(NbLayers)]+\n",
    "    model1 = injectionModel(model, speclist, deltaspecList, params) \n",
    "    print('mutateIndividualModel :')\n",
    "    return model1, speclist\n",
    "\n",
    "def copyModelFromInitial(originalModelPath, IndividuPath):\n",
    "    shutil.copyfile(originalModelPath + 'current_structure.str', IndividuPath + 'current_structure.str')\n",
    "    shutil.copyfile(originalModelPath + 'scalerY.pkl', IndividuPath + 'scalerY.pkl')\n",
    "    shutil.copyfile(originalModelPath + 'scaledY.pkl', IndividuPath + 'scaledY.pkl')  \n",
    "    \n",
    "def createResidues(X, X_scaler, model, y_scaler, Ycall, generate_errors):\n",
    "    Y_size = X.shape[0]\n",
    "    print(\"creating the residus for the boosting\")\n",
    "    Y_ordre1 = numpy.zeros(Ycall.size); \n",
    "    if (generate_errors):        \n",
    "        for i in range(Y_size):\n",
    "            normalizedData = X_scaler.transform(X[i].reshape(1, -1))\n",
    "            normalizedPrediction = model.predict(normalizedData)\n",
    "            outputData = y_scaler.inverse_transform(normalizedPrediction)\n",
    "            Y_ordre1[i] = outputData[0,0] - Ycall[i]\n",
    "        return Y_ordre1\n",
    "    else:\n",
    "        return Y_ordre1 \n",
    "    \n",
    "def ComputeIndividualNote(individualpath, cnfigName, params):\n",
    "    file = individualpath + '/' + cnfigName + '.json'\n",
    "    os.chdir(individualpath)\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()              \n",
    "    model = model_from_json(loaded_model_json)              ### chargment du model initial (1)\n",
    "    file = individualpath + '/' + cnfigName + '.hdf5'\n",
    "    model.load_weights(file)  ### chargment du model initial (2)\n",
    "    note = Compute_Note(model, params)\n",
    "    return note\n",
    "\n",
    "def Idealpopulation(nstep):\n",
    "    if (nstep < 4): \n",
    "        return 3 * nstep + 2\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injectionModel(model, modelspecList, deltaspecList, params, optimizer) : \n",
    "    modelspeclist2 = [[modelspecList[i][0], modelspecList[i][1] + deltaspecList[i]] for i in range(len(modelspecList))]\n",
    "    model2 = buidModel(modelspeclist2, params)\n",
    "    \n",
    "    ## debut de la recopie des poids\n",
    "    weights1 = model.get_weights()\n",
    "    param_list = list(map(lambda k: k.shape, weights1))\n",
    "    weights2 = model2.get_weights()\n",
    "    \n",
    "  ## debut de la recopie des poids\n",
    "    for iparam in range(len(param_list)):\n",
    "        w = subcopy(weights1[iparam], weights2[iparam], param_list[iparam])\n",
    "        weights2[iparam] = w  \n",
    "     \n",
    "    G = 1\n",
    "    #model2 = keras.utils.multi_gpu_model(model2, gpus = G)\n",
    "    model2.compile(loss = 'mse', optimizer = optimizer, metrics = [\"mse\"])\n",
    "    model2.set_weights(weights2) \n",
    "    \n",
    "    return model2\n",
    "\n",
    "def mutateIndividualModel(model, specList, params, optimizer):\n",
    "    NbLayers = len(specList)\n",
    "    deltaNbNeuronList = BulleListPar(0, NbLayers)\n",
    "    print('mutateIndividualModel : adding neurons', deltaNbNeuronList)\n",
    "    print('specList=', specList)\n",
    "    print('deltaNbNeuronList=', deltaNbNeuronList)\n",
    "    model2 = injectionModel(model, specList, deltaNbNeuronList, params, optimizer) \n",
    "    specList2 = [[specList[i][0], specList[i][1] + deltaNbNeuronList[i]] for i in range(NbLayers)]\n",
    "    \n",
    "    return model2, specList2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OP = {'lr':0.001, 'momentum':0.01, 'decay':0.99, 'nesterov':False}\n",
    "# opt = Optimizer('SGD', OP)\n",
    "\n",
    "# joblib.dump(opt, 'C:/Users/Sergey/Desktop/Natixis/Optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one_task(task, d_values):\n",
    "    value = task.execute()\n",
    "    d_values[task.IndividuPath] = value\n",
    "\n",
    "def EXECUTE_ALL_TASKS(LIST):\n",
    "    dict_values = {}\n",
    "    threads = []\n",
    "    \n",
    "    for task in LIST:\n",
    "        x = threading.Thread(target = execute_one_task, args = (task, dict_values))\n",
    "        x.start()\n",
    "        threads.append(x)\n",
    "    for x in threads:\n",
    "        x.join()\n",
    "        \n",
    "    return dict_values\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, opt_type, params):\n",
    "        \n",
    "        assert type(params) is dict, \"params must be a dict\" \n",
    "        assert opt_type in {'SGD', 'Adam'}, \"this type of optimizer is not supported\" \n",
    "        \n",
    "        self.type = opt_type\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'momentum', 'decay', 'nesterov'}, 'Illegal name of parameter'\n",
    "        elif self.type == 'Adam':\n",
    "            for param in params:\n",
    "                assert param in {'lr', 'beta_1', 'beta_2', 'epsilon', 'decay', 'amsgrad'}, 'Illegal name of parameter' \n",
    "        \n",
    "        self.params = params\n",
    "    \n",
    "    @classmethod   \n",
    "    def from_file(self, address):\n",
    "        \n",
    "        load = joblib.load(address)\n",
    "        \n",
    "        return load\n",
    "\n",
    "    def create_instance(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            return SGD(**self.params)\n",
    "        elif self.type == 'Adam':\n",
    "            return Adam(**self.params)\n",
    "        \n",
    "    def save_to_individu(self, address):\n",
    "        \n",
    "        try:\n",
    "            joblib.dump(self, address)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    def introduce_mutation(self):\n",
    "        \n",
    "        if self.type == 'SGD':\n",
    "            pass\n",
    "        elif self.type == 'Adam':\n",
    "            pass\n",
    "        \n",
    "        r = np.random.rand()\n",
    "        \n",
    "        if r < 0.15:\n",
    "            self.params['lr'] *= 2.0\n",
    "        elif r < 0.55:\n",
    "            self.params['lr'] *= 1.0\n",
    "        else:\n",
    "            self.params['lr'] *= 0.5\n",
    "            \n",
    "            \n",
    "class Task:\n",
    "    def __init__(self, ModelPath, individuPath, Ycall, cnfigName, train_gen, val_gen, optimizer, **fit_params):\n",
    "        \n",
    "        #assert type(model) is list\n",
    "        assert type(optimizer) is Optimizer \n",
    "        \n",
    "        self.ModelPath = ModelPath \n",
    "        self.IndividuPath = individuPath\n",
    "        self.cnfigName = cnfigName \n",
    "        \n",
    "        self.Ycall = Ycall\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.fit_params = fit_params\n",
    "        \n",
    "        self.train_generator = train_gen\n",
    "        self.val_generator = val_gen\n",
    "        \n",
    "    def __evaluate(self, model):\n",
    "        score = model.evaluate_generator(self.val_generator)\n",
    "        return score[0]\n",
    "        \n",
    "    def execute(self):\n",
    "        \n",
    "        session = tf.Session()\n",
    "        K.set_session(session)\n",
    "    \n",
    "        with session.as_default():\n",
    "            with session.graph.as_default():\n",
    "                \n",
    "                opt = self.optimizer.create_instance()\n",
    "                model, specList, y_scaler, y_scaled = loadInvidualModel(self.ModelPath, self.cnfigName)\n",
    "                model2, specList2 = mutateIndividualModel(model, specList, params, opt)\n",
    "                \n",
    "                history = model2.fit_generator(generator = self.train_generator, validation_data = self.val_generator,\\\n",
    "                                           **self.fit_params)\n",
    "                \n",
    "                evaluation = self.__evaluate(model2)#mm.evaluate(self.evaluate_generator)\n",
    "                \n",
    "                generate_errors = False\n",
    "                Y_ordre1 = createResidues(params.X, X_scaler, model2, y_scaler, self.Ycall, generate_errors)\n",
    "                saveIndividualModel(model2, X_scaler, y_scaler, y_scaled, Y_ordre1, specList, self.IndividuPath, self.cnfigName)  \n",
    "                self.optimizer.save_to_individu(self.IndividuPath + '/Optimizer')   \n",
    "        \n",
    "        del model\n",
    "        del model2\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        gc.collect()\n",
    "        session.close()\n",
    "                \n",
    "        return  evaluation# ,history.history\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReinforceOptimalityWithGenetic(train_generator, val_generator, pkgNameOriginal, pkgNameBut, InitialNbIndividual, populationctrlFunc, nbChildAllowed,\\\n",
    "                         nbloop, Ycall, cnfigName, params, save=True, generate_errors = False):\n",
    "    \n",
    "    originalModelPath = params.PATH + \"/\" + pkgNameOriginal   \n",
    "    filename = originalModelPath + '/' + cnfigName + 'scaledY.pkl'\n",
    "    y_scaled = joblib.load(filename)\n",
    "    filename = originalModelPath + '/' + cnfigName +'scalerY.pkl'\n",
    "    y_scaler = joblib.load(filename)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split( \\\n",
    "                params.X_scaled, y_scaled, test_size = 0.10, random_state = 3)\n",
    "    \n",
    "    SimuPath = params.PATH + \"/\" + pkgNameBut \n",
    "    if not(os.path.isdir(SimuPath)):\n",
    "        os.makedirs(SimuPath, 0o777)\n",
    "        \n",
    "    SimuPath = SimuPath + '/loop'\n",
    "    \n",
    "    if (os.path.isdir(SimuPath)):\n",
    "        shutil.rmtree(SimuPath)\n",
    "        \n",
    "    os.makedirs(SimuPath, 0o777 ) \n",
    "    precedingIndividualPathList =[]\n",
    "    \n",
    "    for iTimeStep in range(nbloop):\n",
    "        TimePath = SimuPath + \"/\" + \"time\" + str(iTimeStep) \n",
    "        \n",
    "        if not(os.path.isdir(TimePath)):\n",
    "                os.makedirs(TimePath, 0o777)\n",
    "                print(\"created : \",TimePath)\n",
    "                \n",
    "        LIST_OF_TASKS = []        \n",
    "        \n",
    "        if (iTimeStep == 0) :\n",
    "            for individual in range(InitialNbIndividual):\n",
    "                IndividuPath = TimePath +  \"/individu\" + str(individual)\n",
    "                \n",
    "                if not(os.path.isdir(IndividuPath)):\n",
    "                        os.makedirs(IndividuPath, 0o777)\n",
    "                        print(\"created : \",IndividuPath)\n",
    "                        \n",
    "                ## ....traitement\n",
    "                #train_generator = CREATE_GENERATOR_TRAIN()\n",
    "                #val_generator = CREATE_GENERATOR_VAL()\n",
    "                optimizer = Optimizer.from_file(params.PATH +'/' + pkgNameBut + '/Optimizer')\n",
    "                optimizer.introduce_mutation()\n",
    "                \n",
    "                train_gen = copy.deepcopy(train_generator) #DataGenerator_N()\n",
    "                val_gen = copy.deepcopy(val_generator)# DataGenerator_N()\n",
    "                \n",
    "                checkpointer = keras.callbacks.ModelCheckpoint(filepath = IndividuPath + \"/BestWeights.hdf5\",verbose=1,\\\n",
    "                                                   save_best_only=True , monitor = 'loss')\n",
    "                \n",
    "                fit_params = {'epochs':2, 'verbose':1, 'callbacks': [checkpointer]}\n",
    "                \n",
    "                task = Task(originalModelPath, IndividuPath, Ycall, cnfigName, train_gen, val_gen, optimizer, **fit_params)\n",
    "                \n",
    "                LIST_OF_TASKS.append(task)\n",
    "                \n",
    "                #model, specList, y_scaler, y_scaled = loadInvidualModel(originalModelPath, cnfigName)\n",
    "                #model2, specList2 = mutateIndividualModel(model, specList, params)\n",
    "                #convergeIndividualModel(model2, params)\n",
    "                #Y_ordre1 = createResidues(params.X, X_scaler, model2, y_scaler, Ycall, generate_errors)\n",
    "                #saveIndividualModel(model2, X_scaler, y_scaler, y_scaled, Y_ordre1, specList, IndividuPath, cnfigName)   \n",
    "                ## .. Fin du traitement\n",
    "                \n",
    "                precedingIndividualPathList.append(IndividuPath)                     \n",
    "        else :\n",
    "            \n",
    "            invidualpathList = []\n",
    "            individuNumero = 0\n",
    "            \n",
    "            for individualpath in precedingIndividualPathList:                            \n",
    "            \n",
    "                #model, specList, y_scaler, y_scaled = loadInvidualModel(individualpath, cnfigName)\n",
    "                \n",
    "                for child in range(nbChildAllowed): \n",
    "                    \n",
    "                    train_gen = copy.deepcopy(train_generator) #DataGenerator_N()\n",
    "                    val_gen = copy.deepcopy(val_generator)# DataGenerator_N()\n",
    "                    \n",
    "                    checkpointer = keras.callbacks.ModelCheckpoint(filepath = individualpath + \"/BestWeights.hdf5\",verbose=1,\\\n",
    "                                                   save_best_only=True , monitor = 'loss')\n",
    "                    \n",
    "                    fit_params = {'epochs':2, 'verbose':1, 'callbacks':[checkpointer]}\n",
    "                    optimizer = Optimizer.from_file(individualpath + '/Optimizer')\n",
    "                    optimizer.introduce_mutation()\n",
    "                    \n",
    "                    childPath = pathCreatechild(TimePath, individualpath, individuNumero)\n",
    "                    individuNumero += 1\n",
    "                    \n",
    "                    task = Task(individualpath, childPath, Ycall, cnfigName, train_gen, val_gen, optimizer, **fit_params)\n",
    "                    \n",
    "                    LIST_OF_TASKS.append(task)\n",
    "                    #model2,specList2 = mutateIndividualModel(model,specList,params)\n",
    "                    #convergeIndividualModel(model2,params)\n",
    "                    #Y_ordre1 = createResidues(params.X,X_scaler,model2,y_scaler,Ycall,generate_errors)\n",
    "                    #saveIndividualModel(model2,X_scaler,y_scaler,y_scaled,Y_ordre1,specList2,childPath,cnfigName)\n",
    "                    \n",
    "                    invidualpathList.append(childPath)\n",
    "                precedingIndividualPathList = invidualpathList\n",
    "                \n",
    "        NOTES_INDIVIDUS = EXECUTE_ALL_TASKS(LIST_OF_TASKS)\n",
    "        \n",
    "        ## .. Fin du traitement\n",
    "        ## .. Debut Selection Naturelle\n",
    "        ListOnote = []\n",
    "        \n",
    "        #invidualpathList = precedingIndividualPathList\n",
    "        \n",
    "        #for individualpath in invidualpathList :\n",
    "        #    note = ComputeIndividualNote(individualpath,cnfigName,params)\n",
    "        #    ListOnote.append(note)\n",
    "        \n",
    "        SORTED_INDIVIDUS = sorted(NOTES_INDIVIDUS, key = NOTES_INDIVIDUS.__getitem__)\n",
    "        \n",
    "        N_survive = populationctrlFunc(iTimeStep)\n",
    "        bestNoteIndividus = SORTED_INDIVIDUS[:N_survive]\n",
    "        worstNoteIndividus = SORTED_INDIVIDUS[N_survive:]\n",
    "        \n",
    "        #worstNoteIndexlist,bestNoteIndexlist = ComputeBestNoteIndexlist(ListOnote,populationctrlFunc,iTimeStep)\n",
    "        print('ListOnote=', SORTED_INDIVIDUS)\n",
    "        print('bestNoteIndexlist=', bestNoteIndividus)\n",
    "        print('worstNoteIndexlist=', worstNoteIndividus)\n",
    "        \n",
    "        for individu in worstNoteIndividus : \n",
    "            killIndividual(individu)\n",
    "            \n",
    "        precedingIndividualPathList = [individu for individu in bestNoteIndividus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0923 11:50:06.370736 123145490513920 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0923 11:50:06.372243 123145495769088 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0923 11:50:06.374980 123145490513920 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0923 11:50:06.376019 123145501024256 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0923 11:50:06.378155 123145495769088 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0\n",
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu0\n",
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu2\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Test\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 11:50:06.974448 123145501024256 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0923 11:50:06.974931 123145495769088 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0923 11:50:06.975327 123145490513920 deprecation_wrapper.py:119] From /Users/olivier/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 10) (193, 23) (193, 10)\n",
      "(10,) (23,) (10,)\n",
      "(10, 20) (23, 25) (10, 20)\n",
      "(20,) (193, 10) (193, 23) (193, 10)\n",
      "(25,) (20,)\n",
      "(20, 10) (25, 23) (20, 10)\n",
      "(10, 30) (23, 32) (10, 30)\n",
      "(10,) (23,) (10,)\n",
      "(10, 20) (23, 25) (10, 20)\n",
      "(20,)(193, 10) (193, 23) (193, 10)\n",
      " (25,) (20,)(10,) (23,) (10,)\n",
      "(10, 20) (23, 25) (10, 20)\n",
      "(20,) (25,) (20,)\n",
      "(20, 10) (25, 23) (20, 10)\n",
      "(10, 30) (23, 32) (10, 30)\n",
      "(30,) (32,) (30,)\n",
      "(10, 30) (23, 32) (10, 30)\n",
      "(30, 10) (32, 11) (30, 10)\n",
      "(30,) (32,)\n",
      "(20, 10) (25, 23) (20, 10)\n",
      "(10, 30) (23, 32) (10, 30)\n",
      "(30,) (32,) (30,)\n",
      "(10, 30) (23, 32)  (30,)\n",
      "(10, 30)(10, 30)\n",
      "(30, 10) (32, 11) (30, 10)\n",
      "(10,) (11,) (10,)\n",
      "(10, 1) (11, 1) (10, 1)\n",
      "(1,) (1,) (1,)\n",
      " (23, 32) (10, 30)\n",
      "(30, 10) (32, 11) (30, 10)\n",
      "(10,) (11,) (10,)\n",
      "(10, 1) (11, 1) (10, 1)\n",
      "(1,) (1,) (1,)\n",
      "(10,) (11,) (10,)\n",
      "(10, 1) (11, 1) (10, 1)\n",
      "(1,) (1,) (1,)\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 7.1375 - mean_squared_error: 7.1375 - val_loss: 7.2119 - val_mean_squared_error: 7.2119loss: 11.9433 303/1500 [=====>........................] - ETA: 15s - loss: 11 598/1500 [==========>...................] - ETA: 6s - loss: 10.9258 - mean_squar 638/1500 [===========>..................] - ETA: 6s - loss: 10.9441 - mean_squared_error: 1 486/1500 [========>.....................] - ETA: 10s - loss: 7.1530 - m 714/1500 [=============>................] - ETA: 5s - loss: 10.9416 - mean_squared_error: 728/1500 [=============>................] - ETA: 5s - ETA: 8s - loss: 11. 722/1500 [=============>................] - ETA: 7s - loss: 7.1370 - mean_squared_error - ETA: 7s - loss: 11.7574 - mean_squared_error: 1 - ETA: 7s - loss:  792/1500 [==============>...............] - ETA: 6s - loss: 11.6309 - mean_squared 859/1500 [================>...1170/1500 [======================>.......] - ETA: 2s1010/1500 [===================>..........] - ETA: 4s - loss: 11.5613 - mean_1222/1500 [=1421/1500 [===========================>..] -\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.13746, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu0/BestWeights.hdf5\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 16s 11ms/step - loss: 10.5423 - mean_squared_error: 10.5423 - val_loss: 9.7739 - val_mean_squared_error: 9.7739\n",
      "\n",
      "Epoch 00001: loss improved from inf to 10.54232, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      " 109/1500 [=>............................] - ETA: 7s - loss: 8.1163 - mean_squared_error: 8.1163Epoch 2/2\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 11.2377 - mean_squared_error: 11.2377 - val_loss: 10.6803 - val_mean_squared_error: 10.6803\n",
      " 671/1500 [============>.................] - ETA: 4s - loss: 9.5962 - mean_squared_error: 9.5962\n",
      "Epoch 00001: loss improved from inf to 11.23769, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu2/BestWeights.hdf5\n",
      " 698/1500 [============>.................] - ETA: 4s - loss: 9.5935 - mean_squared_error: 9.5935Epoch 2/2\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 9.6417 - mean_squared_error: 9.6417 - val_loss: 9.5117 - val_mean_squared_error: 9.5117\n",
      "\n",
      "Epoch 00002: loss improved from 10.54232 to 9.64170, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "1486/1500 [============================>.] - ETA: 0s - loss: 10.6288 - mean_squared_error: 10.6288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 2, in execute_one_task\n",
      "    value = task.execute()\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 114, in execute\n",
      "    history = model2.fit_generator(generator = self.train_generator, validation_data = self.val_generator,                                           **self.fit_params)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 234, in fit_generator\n",
      "    workers=0)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1472, in evaluate_generator\n",
      "    verbose=verbose)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 330, in evaluate_generator\n",
      "    generator_output = next(output_generator)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 601, in get\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/six.py\", line 693, in reraise\n",
      "    raise value\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 595, in get\n",
      "    inputs = self.queue.get(block=True).get()\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 644, in get\n",
      "    raise self._value\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 2, in execute_one_task\n",
      "    value = task.execute()\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 114, in execute\n",
      "    history = model2.fit_generator(generator = self.train_generator, validation_data = self.val_generator,                                           **self.fit_params)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 234, in fit_generator\n",
      "    workers=0)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1472, in evaluate_generator\n",
      "    verbose=verbose)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 330, in evaluate_generator\n",
      "    generator_output = next(output_generator)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 601, in get\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/six.py\", line 693, in reraise\n",
      "    raise value\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 595, in get\n",
      "    inputs = self.queue.get(block=True).get()\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 644, in get\n",
      "    raise self._value\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "ListOnote= ['/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1']\n",
      "bestNoteIndexlist= ['/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1']\n",
      "worstNoteIndexlist= []\n",
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu3\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,)(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      " (1,)\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 26s 17ms/step - loss: 7.9709 - mean_squared_error: 7.9709 - val_loss: 7.8564 - val_mean_squared_error: 7.8564- loss: 7.3601 - mean_squared_error: 263/1500 [====>.........................] - ETA: 17s - loss: 11.2713 - mean_squared_error: 11.271 - ETA: 12s - loss: 7. - ETA: 2:13 - loss: 9.9159 - mean_squared_error 389/1500 [======>.......................] - ETA: 12s - loss: 7.4644 - mean_squ 151/1500 [==>...........................] - ETA: 39s - loss: 9.7181 - mean_squared_ 165/1500 [==>...........................] 348/1500 [=====>........................] - ETA: 15s - loss: 11.1490 - mean_squared_error: 11.1490 - ETA: 37s - loss: 9.6013 - mean_squared_erro 360/1500 [======>.......................] - ETA: 15s - loss: 11.1295 - me 203/1500 [===>..........................] - ETA: 32s - loss: 9.3201 - mean_squared_error:  - ETA: 32s - loss:  454/1500 [========>.....................] - ETA: 13s - loss: 10.8844 - mean_squared_error: 10.8 460/1500 [========>.....................] - ETA:  - ETA: 31s - loss: 9.0077 - mean_squared_error: 9 - ETA: 24s - loss: 9.3243 - mean_ - ETA: 10s - loss: 7.5209 - mean_ 372/1500 [======>.......................] - ETA: 22s - loss: 9.3685 - mean_squared_ 322/1500 [=====>........................] - E 702/1500 [=============>................] - ETA: 9s - loss: 7.3969 - mean_squared_er 641/1500 [===========>..................] - ETA: 11s - loss: 10.6347 -  657/1500 [============>.................] - ETA: 11s - loss: 10.6189 - mean_squared_error: 10.6189 408/1500 [=======>......................] - ETA: 22s - loss: 8.3281  756/1500 [==============>...............] - ETA: 8s - loss: 7.4227 792/1500 [==============>...............] - ETA: 8s - loss: 7.4261 - mean_squared_error: 7.4 475/1500 [========>.....................] - ETA: 20s - loss: 8.1847 - mean_squared_error: 8.1847 796/1500 [==============>...............] 713/1500 [=============>................] - ETA: 10s - loss: 10.5769 - mean_squar - ETA: 10s - loss: 10.5812 - m 844/1500 [==== 614/1500 [===========>..................] - ETA: 16s - loss: 8 - ETA: 15s - loss: 8.2168 - mean_squared_error: 8.2168 - ETA: 7s - loss: 10.2350 - mean_squared_error: 1 930/1500 [=================>........ 990/1500 [==================>...........] - ETA: 6s - loss: 10.2467 - mean_squared_er 720/1500 [=============>................] - ETA: 13s - lo1036/1500 [===================>..........] - ETA: 5s - 872/1500 [================>.............] - ETA: 9s - loss: 8.7475 - mean_squared_error: 81109/1500 [=====================>........] - ETA: 4s - loss: 7.4487 - mean_squared_error: 7. 801/1500 [===============>..............] - ETA: 11s - loss: 8.0494 - mea1114/1500 [=====================>........] - ETA: 4s - loss: 10.1877 - mean_ 910/1500 [=================>............] - ETA: 9s - l - ETA: 10s - loss: 8.0719 - mean_squared_error: 8. 873/1500 [================>.............] - ETA: 10s - loss: 8.0700 - mea 892/1500 [================>.............] - ETA: 10s - loss: 8.0667 - mean_square 898/1500 [================>.............] - ETA: 9s - loss: 8.0635 - 919/1500 [=================>............] - ETA: 9s - loss: 8.0586 - mean_sq 930/1500 [=================>............] - ETA: 9s - loss: 8.0254 - mean_squared_error: 8. - ETA: 3s - loss: 7.3707 - mean_squared_erro1257/1500 [========================>.....] - ETA: 2s - loss: 7.3629 - mean_squared_1264/1500 [========================>.....] - ETA: 1046/1500 [===================>..........]1286/1500 [========================>.....] - ETA: 2s - loss: 10.1021 - mean_squared_error: 10.10 - ETA: 6s - loss: 8.6357 - mean_squared_error: 8.61304/1500 [=========================>....] - ETA: 2s - loss: 7.3703 - mean_square 998/1500 [==================>.......... - ETA: 8s - loss: 8.0021 - mean_squared_error: 8.01303/1500 [=========================>....] - ETA: 2s - loss: 10.1135 - mean_squared_error1307/1500 [========1123/1500 [=====================>........] - ETA: 5s - loss1153/1500 [======================>.......] - ETA:1202/1500 [=======================>......] - ETA1308/1500 [=========================>....] - ETA: 2s - loss: 8.5512 - mean_squared_error:1326/1500 [======================\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.97093, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s 18ms/step - loss: 7.3553 - mean_squared_error: 7.3553 - val_loss: 7.3740 - val_mean_squared_error: 7.3740\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.35527, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "1500/1500 [==============================] - 27s 18ms/step - loss: 10.0364 - mean_squared_error: 10.0364 - val_loss: 9.1872 - val_mean_squared_error: 9.1872\n",
      "\n",
      "Epoch 00001: loss improved from inf to 10.03643, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  49/1500 [..............................] - ETA: 9s - loss: 7.4649 - mean_squared_error: 7.4649 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-81:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 2, in execute_one_task\n",
      "    value = task.execute()\n",
      "  File \"<ipython-input-13-fe2093395268>\", line 114, in execute\n",
      "    history = model2.fit_generator(generator = self.train_generator, validation_data = self.val_generator,                                           **self.fit_params)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 251, in fit_generator\n",
      "    callbacks.on_epoch_end(epoch, epoch_logs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\", line 79, in on_epoch_end\n",
      "    callback.on_epoch_end(epoch, logs)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\", line 446, in on_epoch_end\n",
      "    self.model.save(filepath, overwrite=True)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\", line 1090, in save\n",
      "    save_model(self, filepath, overwrite, include_optimizer)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\", line 379, in save_model\n",
      "    f = h5dict(filepath, mode='w')\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/keras/utils/io_utils.py\", line 186, in __init__\n",
      "    self.data = h5py.File(path, mode=mode)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\", line 269, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\n",
      "  File \"/Users/olivier/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\", line 105, in make_fid\n",
      "    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 98, in h5py.h5f.create\n",
      "OSError: Unable to create file (unable to truncate a file which is already open)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 29s 19ms/step - loss: 8.5040 - mean_squared_error: 8.5040 - val_loss: 8.2726 - val_mean_squared_error: 8.2726oss: 7.4673 -  149/1500 [=>............................] - ETA: 10s - loss: 7.7903 - mean_squared_error: 7.7 269/1500 [====>.........................] - ETA: 10s - loss: 7.5728 - mean_squa\n",
      "\n",
      "Epoch 00001: loss improved from inf to 8.50396, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      " 227/1500 [===>..........................] - ETA: 10s - loss: 7.5119 - mean_squared_error: 7.5119Epoch 2/2\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 7.7865 - mean_squared_error: 7.7865 - val_loss: 7.8152 - val_mean_squared_error: 7.8152oss: 587/1500 [==========>...................] - ETA: 7s - loss:  535/1500 [=========>....................] - ETA: 7 - ETA: 5s - loss: 7.6712 1088/1500 [================1106/1500 [====================1281/1500 [========================>\n",
      "\n",
      "Epoch 00002: loss improved from 7.97093 to 7.78655, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 7.2453 - mean_squared_error: 7.2453 - val_loss: 7.3846 - val_mean_squared_error: 7.3846\n",
      "\n",
      "Epoch 00002: loss improved from 7.35527 to 7.24526, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 8.2654 - mean_squared_error: 8.2654 - val_loss: 8.1781 - val_mean_squared_error: 8.1781\n",
      "\n",
      "Epoch 00002: loss improved from 8.50396 to 8.26536, saving model to /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time0/individu1/BestWeights.hdf5\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "creating the residus for the boosting\n",
      "saveIndividualModel model2 at : /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "ListOnote= ['/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2', '/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1', '/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0']\n",
      "bestNoteIndexlist= ['/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2', '/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1', '/Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0']\n",
      "worstNoteIndexlist= []\n",
      "created :  /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu0\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu1\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu2\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu3\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu4\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu5\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu6\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu7\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu8\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu9\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu10\n",
      "pathCreatechild= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time2/individu11\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu0\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu1\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n",
      "loadInvidualModel :IndividuPath= /Users/olivier/keras/NeuralPricing/A_YetiPhoenix_VolLoc/New_Simu_Test/loop/time1/individu2\n",
      "mutateIndividualModel : adding neurons [13, 5, 2, 1]\n",
      "specList= [[0, 10], [1, 20], [2, 30], [0, 10]]\n",
      "deltaNbNeuronList= [13, 5, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fa2989e6748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1471, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fa298a63828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1471, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fa298a8c898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1471, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fa298c98dd8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/olivier/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1471, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23)(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25)  (23, 25)(193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25)  (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 25)\n",
      "(25,) (25,) (23, 32)(25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      " (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)(193, 23)\n",
      " (193, 23) (193, 23)\n",
      "(23, 25)(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      " (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "(193, 23) (193, 23) (193, 23)\n",
      "(23,) (23,) (23,)\n",
      "(23, 25) (23, 25) (23, 25)\n",
      "(25,) (25,) (25,)\n",
      "(25, 23) (25, 23) (25, 23)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32,) (32,) (32,)\n",
      "(23, 32) (23, 32) (23, 32)\n",
      "(32, 11) (32, 11) (32, 11)\n",
      "(11,) (11,) (11,)\n",
      "(11, 1) (11, 1) (11, 1)\n",
      "(1,) (1,) (1,)\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "   1/1500 [..............................] - ETA: 9:51 - loss: 5.3639 - mean_squared_error: 5.3639Epoch 1/2\n",
      "Epoch 1/2\n",
      "   1/1500 [..............................] - ETA: 10:14 - loss: 5.1945 - mean_squared_error: 5.1945Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 250/1500 [====>.........................] - ETA: 1:05 - loss: 7.7173 - mean_squared_error: 7.717394oss: 7.8215 - mean_squared_error: 7.8215_error: 8.1234oss: 7.8893 - mean_squared_error: 7.889339.........................] - ETA: 2:00 - loss: - ETA: 8:21 - loss: 8.8270 - mean_squared_error: 8.8  96/1500 [>.............................] - ETA: 1:02 - loss: 7.7964 - mean_squared_error: 7.79  18/1500 [..............................] - ETA: 5:47 - loss: 7.5604   21/1500 [..............................] - ETA: 5:06 - loss: 7.4732 - mean_squared_er  16/1500 [..............................] - ETA: 6:39 - loss: 8.3183 - mean_squared_error: 8.318  55/1500 [>.............................] - ETA: 1:57 - loss: 8.6017 - mean_squared_error: 8.  66/1500 [>.............................] - ETA: 1:38 - loss: 9.1498 - mean_squared_er 102/1500 [=>......................... 107/1500 [=>............................] - ETA: 1:03 - loss: 8.16  27/1500 [..............................] - ETA: 4:21 - loss: 7.5707 - ETA: 1:04 - loss: 8.1689 - mean_squared_error: 8.  38/1500 [..............................] - ETA: 3:06 - loss: 8.9644 - mean_squared_error: 8  20/1500 [..............................] - 114/1500 [=>............................] - ETA: 1:04 - loss: 8.1404 - mean_squared_error: 8  17/1500 [..............................] - ETA: 6:56 - loss:   13/1500 [..............................]  16/1500 [..............................]   50/1500 [>.............................] - ETA: 2:34 - loss: 8.9464 - mean_squared_erro  17/1500 [..............................] - ETA: 6:47 - loss: 13.5678 - mean_squared  29/1500 [..............................] - ETA: 3:32 - loss: 8.0534 - mean_squared_error:   26/1500 [..............................] - ETA: 5:01 - loss: 8.0117 - mean_squared_error: 8.0117  33/1500 [............................. - ETA: 4:05 - loss: 7.9591 - mean_squared_error:   59/1500 [>.............................] - ETA: 2:17 - loss: 9.1642 - mean_squared_error: 9.1642  39/1500 [..............................] -   30/1500 [..............................] - ETA: 4:31 - loss: 8.2553 - mean_squared_er  33/15  58/1500 [>.............................] - ETA: 2:37 - loss: 7.4088 - mean_sq  60/1500 [>.............................] - ETA: 2:33 - loss: 7.4849 - mean_squared_  39/1500 [..............................] - ETA: 3:43 - loss: 7.7520  33/1500 [..............................] - ETA: 4:10 -   62/1500 [>.............................] - ETA: 2:31 - loss: 6.7352 - mean_squared_er 105/1500 [=>............................] - ETA: 1:2  46/1500 [........................... - ETA: 3:20 - loss: 7.9705 - mean_square  67/1500 [>.............................] - ETA: 2:24 - loss: 6.7749 - mean_squared 112/1500 [=>............................] - ETA: 1:26 - loss: 8.9647 - mean_squared_error: 8. - ETA:    - ETA: 2:34 - loss: 12.4785 - mean_squar 128/1500 [=>............................] - ETA: 1:23 - loss: 8.8201 - mean_squared_error: 8.8201 - ETA: 2:36 - loss: 7.9377 - mean_  66/1500 [>.............................] - ETA: 2 106/1500 [=>........................... - ETA: 1:41 - loss: 8.3733 - mean_squared_erro  70/1500 [>.............................] - ETA: 2:24 - los  78/15  83/1500 [>.............................] - ETA: 2:18 - loss: 8.1165 - mean_s  76/1500 [>............... 100/1500 [=>............................  84/1500 [>.............................] - ETA: 2:14 - loss: 11.6854 - mean_sq 155/1500 [==>...........................] - ETA: 1:18 - loss: 8. 113/1500 [=>............................] - ETA: 1:51 - loss: 7.3183 - mean_squared_error: 7.318  94/1500 [>.............................] - ETA: 2:11 - loss: 8.0233 - mean_squared_e - ETA: 2:10 - loss: 7.3499 - mean 115/1500 [=>............................] - ETA: 1:51 - loss: 7.3112 - mean_squared_  98/1500 [>.............................] - ETA: 2:09 - loss: 7.388 140/1500 [=>............................] - ETA: 1:29 - loss: 8.1904 - mean_square  90/1500 [>.............................] - ETA: 2:20 - loss: 8.2274 - mean_squared_error: 8.22  92/1500 [>............................ - ETA: 2:10 - loss: 11.4875 - mean_squar 157/1500 [==>...........................] - ETA: 1:21 - loss: 8.9407 - mean_squared_erro 120/1500 [=>............................] - ETA: 1:48 - - ETA: 2:13 - loss: - ETA: 1:53 - loss: 7.2642  125/1500 [=>............................] - ETA: 1:47 - loss: 7.3367 -  - ETA: 1:28 - loss: 8.2651 - mean_sq 165/1500 [==>...........................] - ETA: 1:21 - loss: 9.0172 - mean_squared 166/1500 [==>...........................] - ETA: 1:21 - loss: 9.0076 - mean_squared_error: 9.0076  143/1500 [=>............................] - ETA: 1:41 - loss: 7.7683 - mean_ 170/1500 [==>...........................] - ETA: 1:23 - loss: 8.3159 - mean_squared_error: 8.31 187/1500 [==>...........................] - ETA: 1 219/1500 [===>..........................] 127/1500 [=>............................] - ETA: 1:56 - loss: 7.6893 - mean_squared_error: 7.6893 - ETA: 1:04 - loss: 7.7404 - mean_squared_error: 7 148/1500 [=>............................] - ETA: 1:39 - loss: 7.7733 - mean_squared_error - ETA: 1:41 - loss: 7.5138 - mean_squ - ETA: 1:40 - loss: 7.5402 - mean_squar 194/1500 [==>...........................] - ETA: 1:15 - loss: 8.4670 - mean_squared_error: 8.4670 130/1500 [=>............................] - ETA: 1:55 - loss: 7.6425 - mean_squared_erro 129/1500 [=>............................] - ETA: 1:54 - loss: 8.3048 - mean_squared_error:  195/1500 [==>...........................] - ETA: 1:15 - loss: 8.4856 - mean_squared_error: 8.4856 223/1500 [===>..........................] - ETA: 1:04 - loss: 7.8194 - mean_squared_error: 7.8194 - ETA: 1:40 - loss: 7.5365 - mean_square 128/1500 [=>......................... - ETA: 1:56 - loss: 8.1083 - mean_squar 225/1500 [===>..........................] - ETA: 1:05 - loss: 7.7940 - mean_squared_error: 7.7940 197/1500 [==>.......................... - ETA: 1:16 - loss: 8.5048 - mean_squared_error: 8.5 180/1500 [==>.. - ETA: 1:19 - loss: 9.0427 - mean_ 135/1500 [=>............................] - ETA: 1:53 - loss: 8.3642 - mean_squared_error: 8.3 200/1500 [===>..........................] - ETA: 1:16 - loss: 8 194/1500 [==>...........................] - ETA: 1:20 - loss: 8.9543 - mean_sq 132/1500 [=>............................] - ETA: 1:55 - loss: 11.4804 - mean 139/1500 [=>............................] - ETA: 1:53 - loss: 8.2758 - mean_squa 143/1500 [=>........................... - ETA: 1:52 - loss: 7.7307 - mean_squared_erro 188/1500 [==>.......................... - ETA: 1:23 - loss: 8.1613 -  237/1500 [===>..........................] 145/1500 [=>............................] - ETA: 1:52 - loss: 7.7339 - mean_squared_error: 7.7339 - ETA: 1:05 - loss: 7.7464 - mean_squared_error: 7.7 166/1500 [==>...........................] - ETA: 1:38 - loss: 7.7821 - mean_squared_ 137/1500 [=>............................] - ETA: 1:54 - loss: 11.2518 - mean_squared_error: 11.2518 - ETA: 1:05 - loss: 7.7670 - mean_squared_erro 191/1500 [==>.......................... - ETA: 1:50 - loss: 7.0781 - 203/1500 [===>..........................] - ETA: 1:20 - loss: 8.8671 - mean_square 204/1500 [===>......................... - ETA: 1:38 - loss: 7.3433 - mean_squared_err 147/1500 [=>............................] - ETA: 1:52 - loss: 8.2186 - mean_squared_erro 214/1500 [===>..........................] - ETA: 1:16 - loss: 8.3454 - mean_squared_ 172/1500 [==>...........................] - ETA: 1:38 - loss: 7.3307 - mean_ 173/1500 [==>...........................] - ETA: 1:37 - loss: 7.8392 - mean_squared_error:  155/1500 [==>...........................] - ETA: 1:49 - loss: 7.0940 - mean_squared_error: 7.0940 164/1500 [==>...........................] - ETA: 1:41 - loss: 7.6119 - mean_squared_error: 7.6 247/1500 [===>..........................] - ETA: 1:05 - loss: 7.7425 - mean_ 248/1500 [===>..........................] - ETA: 1:05 - loss: 7.7394 - me 155/1500 [==>...........................] - ETA: 1:50 - loss: 7.8197 - mean_squared_error: 7 202/1500 [===>..........................] - ETA: 1:22 - loss: 8.0146 - mean_squared_error: 212/1500 [===>..........................] - ETA: 1:19 - loss: 8.7983 - mean_squared_error: 8.7983"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 189/1500 [==>...........................] - ETA: 1:36 - loss: 7.7187 - mean_squared_error: 7.718762oss: 7.6984 - mean_squared_error: 7.6984_error: 8.0293 152/1500 [==>...........................] - ETA: 1:48 - loss: 11.1490 - mean_squared_error: 11 251/1500 [====>.........................] - ETA: 1:05 - loss: 7.7028 - mean_squa - ETA: 1:4 254/1500 [====>.........................] - ETA: 1:05 - 183/1500 [==>...........................] - ETA: 1:36 - loss: 7.7107 - mean_squared_error: 7.71 183/1500 [==>...........................] - ETA: 1:37 - loss: 7.4781 - mean_squ 167/1500 [==>...........................] - ETA: 1:46 - loss: 7.1294 - mean_ 185/1500 [==>...........................] - ETA: 1:36 - loss: 7.7321 - mean_squared_error 222/1500 [===>..........................] - ETA: 1:19 - loss: 8.8248 - mean_squared_err 177/1500 [==>...........................] - ETA: 1:40 - loss: 7.7340 - me 233/1500 [===>..........................] - ETA: 1:15 - loss: 8.4212 - mean_squared_error:  163/1500 [==>...........................] - ETA: 1:46 - loss: 11.1179 - mean_squared_error: 11.1179 - ETA: 1:48 - loss: 7.7533 - mean_squared_erro 188/1500 [==>...........................] - ETA: 1:36 - loss: 7.4964 - mean_squared_error: 7 263/1500 [====>.........................] - ETA: 1:05 - loss: 7.7107 - mean_squared_error"
     ]
    }
   ],
   "source": [
    "nbloop =5\n",
    "NbIndividual =3\n",
    "nbChildAllowed = 4\n",
    "\n",
    "pkgNameOriginal = params.LEARNINGBASE_ORIGIN\n",
    "pkgNameBut = params.LEARNINGBASE_BUT\n",
    "InitialNbIndividual =3\n",
    "naturalSelectionPercentage = 0.5\n",
    "\n",
    "a = ReinforceOptimalityWithGenetic(Train_GEN, Val_GEN, pkgNameOriginal, pkgNameBut,\\\n",
    "                    InitialNbIndividual, Idealpopulation,nbChildAllowed, nbloop, Y_Vol, \"Vol\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
